{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import CTransformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINECONE Information for connection\n",
    "API_KEY = \"1447d76e-5c29-49bb-9fce-5cc8642a28d6\"\n",
    "API_ENV = \"gcp-starter\"\n",
    "index_name=\"prabal-knowledge-management-chatbot\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract data\n",
    "class Load_data():\n",
    "    def __init__(self,data):\n",
    "        self.data= data\n",
    "\n",
    "    def load_pdf_data(self):\n",
    "        loader = DirectoryLoader(self.data,\n",
    "                        glob=\"*.pdf\",\n",
    "                        loader_cls=PyPDFLoader)\n",
    "        \n",
    "        documents = loader.load()\n",
    "\n",
    "        return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an object for the load_data class\n",
    "loaded_data_object = Load_data(\"/Users/praba/Documents/GitHub/wikigpt-uca/data/\")\n",
    "loaded_data = loaded_data_object.load_pdf_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create text chunks\n",
    "class Split_text():\n",
    "    def __init__(self, loaded_data):\n",
    "        self.loaded_data = loaded_data\n",
    "    def text_split(self):\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size = 300, chunk_overlap = 10)\n",
    "        chunks = text_splitter.split_documents(self.loaded_data)\n",
    "\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_object = Split_text(loaded_data)\n",
    "text_chunks =chunks_object.text_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of chunks: 28\n"
     ]
    }
   ],
   "source": [
    "print(\"number of chunks:\", len(text_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download embedding model\n",
    "def download_hugging_face_embeddings():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings = download_hugging_face_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "  (2): Normalize()\n",
      ") model_name='sentence-transformers/all-MiniLM-L6-v2' cache_folder=None model_kwargs={} encode_kwargs={}\n"
     ]
    }
   ],
   "source": [
    "print(text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the Pinecone\n",
    "pc =pinecone.init(api_key=API_KEY,\n",
    "              environment=API_ENV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Employee Profile: Alex Thompson  \\nIntroduction:  \\n• Name:  Alex Thompson  \\n• Position:  Machine Learning Engineer  \\n• Department:  AI Research  \\n• Joining Date:  March 15, 2018  \\nProfessional Background:  \\n• Education:  \\n• MSc in Computer Science, Stanford University, 2017', '• BEng in Electrical Engineering, MIT, 2015  \\n• Work Experience:  \\n• Software Engineer, Google Research, 2017 -2018  \\n• Intern, Apple AI Labs, Summer 2016  \\nSkills and Expertise:  \\n• Machine Learning, Deep Learning, Computer Vision  \\nProjects:', '• Developed a real -time object detection system using convolutional neural networks (CNNs).  \\n• Implemented a recommendation engine for personalized content delivery.  \\nAchievements:  \\n• Recognized as \"Employee of the Year\" in 2019 for outstanding contributions to project success.', 'Publications and Contributions:  \\n• Co-authored three research papers on computer vision and machine learning, presented at top \\nconferences.  \\nProfessional Development:  \\n• Completed TensorFlow Certification for Machine Learning.  \\nInterests and Hobbies:', '• Enjoys hiking, photography, and participating in hackathons.  \\nFuture Goals:  \\n• Aspires to lead a research team focusing on cutting -edge AI applications.']\n"
     ]
    }
   ],
   "source": [
    "list_chunks = [text.page_content for text in text_chunks]\n",
    "print(list_chunks[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.pinecone.Pinecone at 0x134d2c80220>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating Embeddings for Each of The Text Chunks & storing\n",
    "Pinecone.from_texts(list_chunks, text_embeddings, index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result [Document(page_content='Employee Profile: Alex Thompson  \\nIntroduction:  \\n• Name:  Alex Thompson  \\n• Position:  Machine Learning Engineer  \\n• Department:  AI Research  \\n• Joining Date:  March 15, 2018  \\nProfessional Background:  \\n• Education:  \\n• MSc in Computer Science, Stanford University, 2017  \\n• BEng in Electrical Engineering, MIT, 2015  \\n• Work Experience:  \\n• Software Engineer, Google Research, 2017 -2018  \\n• Intern, Apple AI Labs, Summer 2016  \\nSkills and Expertise:', metadata={})]\n"
     ]
    }
   ],
   "source": [
    "#If we already have an index we can load it like this\n",
    "retrived_text = Pinecone.from_existing_index(index_name, text_embeddings)\n",
    "\n",
    "query = \"Who is  Alex Thompson\"\n",
    "\n",
    "docs = retrived_text.similarity_search(query, k=1)\n",
    "\n",
    "print(\"Result\", docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employee Profile: Alex Thompson  \n",
      "Introduction:  \n",
      "• Name:  Alex Thompson  \n",
      "• Position:  Machine Learning Engineer  \n",
      "• Department:  AI Research  \n",
      "• Joining Date:  March 15, 2018  \n",
      "Professional Background:  \n",
      "• Education:  \n",
      "• MSc in Computer Science, Stanford University, 2017  \n",
      "• BEng in Electrical Engineering, MIT, 2015  \n",
      "• Work Experience:  \n",
      "• Software Engineer, Google Research, 2017 -2018  \n",
      "• Intern, Apple AI Labs, Summer 2016  \n",
      "Skills and Expertise:\n"
     ]
    }
   ],
   "source": [
    "for i in docs :\n",
    "    print(i.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "Please provide the necessary details for me to assist you effectively. \n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Here's the format to follow:\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Once you've provided the required information, I'll offer you the most helpful answer.\n",
    "\n",
    "Helpful answer:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT=PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
    "chain_type_kwargs={\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"\\nPlease provide the necessary details for me to assist you effectively. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nHere's the format to follow:\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnce you've provided the required information, I'll offer you the most helpful answer.\\n\\nHelpful answer:\\n\", template_format='f-string', validate_template=True)}\n"
     ]
    }
   ],
   "source": [
    "print(chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=CTransformers(model=\"../model/llama-2-7b-chat.ggmlv3.q4_0.bin\",\n",
    "                  model_type=\"llama\",\n",
    "                  config={'max_new_tokens':256})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer=RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=docsearch.as_retriever(search_kwargs={'k': 1}),\n",
    "    return_source_documents=True, \n",
    "    chain_type_kwargs=chain_type_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalQA(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, combine_documents_chain=StuffDocumentsChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, input_key='input_documents', output_key='output_text', llm_chain=LLMChain(memory=None, callbacks=None, callback_manager=None, verbose=False, tags=None, metadata=None, prompt=PromptTemplate(input_variables=['context', 'question'], output_parser=None, partial_variables={}, template=\"\\nPlease provide the necessary details for me to assist you effectively. \\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\nHere's the format to follow:\\n\\nContext: {context}\\nQuestion: {question}\\n\\nOnce you've provided the required information, I'll offer you the most helpful answer.\\n\\nHelpful answer:\\n\", template_format='f-string', validate_template=True), llm=CTransformers(cache=None, verbose=False, callbacks=None, callback_manager=None, tags=None, metadata=None, client=<ctransformers.llm.LLM object at 0x00000134D2D24FA0>, model='../model/llama-2-7b-chat.ggmlv3.q4_0.bin', model_type='llama', model_file=None, config={'max_new_tokens': 256}, lib=None), output_key='text', output_parser=NoOpOutputParser(), return_final_only=True, llm_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], output_parser=None, partial_variables={}, template='{page_content}', template_format='f-string', validate_template=True), document_variable_name='context', document_separator='\\n\\n'), input_key='query', output_key='result', return_source_documents=True, retriever=VectorStoreRetriever(tags=None, metadata=None, vectorstore=<langchain.vectorstores.pinecone.Pinecone object at 0x00000134D5083310>, search_type='similarity', search_kwargs={'k': 1}))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input=input(f\"Input Prompt:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who is Alex ?'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response :  Alex Thompson is a Machine Learning Engineer at AI Research department in the company. He joined the company on March 15, 2018, and his professional background includes earning an MSc in Computer Science from Stanford University in 2017 and a BEng in Electrical Engineering from MIT in 2015. His work experience includes working as a Software Engineer at Google Research from 2017 to 2018 and interning at Apple AI Labs during the summer of 2016. He has skills and expertise in machine learning, computer science, electrical engineering, and software development.\n"
     ]
    }
   ],
   "source": [
    "result=question_answer({\"query\": user_input})\n",
    "print(\"Response : \", result[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
