{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6214dd22",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\"> Pet Finder with Machine Learning </p>\n",
    "### <p style=\"text-align: center;\"> Aglind Reka </p>\n",
    "\n",
    "#### <p style=\"text-align: center;\"> MSc Data Science & Artificial Intelligence </p>\n",
    "#### <p style=\"text-align: center;\"> Machine Learning Algorithms </p> \n",
    "#### <p style=\"text-align: center;\"> 04/04/2023 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177624ee",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4172ba63",
   "metadata": {},
   "source": [
    "\n",
    "The dataset presents pet's characteristics and includes tabular, text and image data. It's come from: https://www.petfinder.my.\n",
    "\n",
    "``The aim is to predict the rate at which a pet is adopted.``\n",
    "\n",
    "Data fields:\n",
    "\n",
    "``index`` - Unique hash ID of pet profile  \n",
    "``Type`` - Dog or Cat  \n",
    "``Age`` - Age of pet when listed, in months  \n",
    "``Gender`` - Gender of pet (Male, Female, Mixed, if profile represents group of pets)  \n",
    "``Color1`` - Color 1 of pet  \n",
    "``Color2`` - Color 2 of pet   \n",
    "``Color3`` - Color 3 of pet   \n",
    "``MaturitySize`` - Size at maturity (Small, Medium, Large, Extra Large, Not Specified)  \n",
    "``FurLength`` - Fur length (Short, Medium, Long, Not Specified)  \n",
    "``Vaccinated`` - Pet has been vaccinated (Yes, No, Not Sure)  \n",
    "``Dewormed`` - Pet has been dewormed (Yes, No, Not Sure)  \n",
    "``Sterilized`` - Pet has been spayed / neutered (Yes, No, Not Sure)  \n",
    "``Health`` - Health Condition (Healthy, Minor Injury, Serious Injury, Not Specified)  \n",
    "``Fee`` - Adoption fee (0 = Free)  \n",
    "``Breed`` - breed of pet (see on the dataset)  \n",
    "``Description`` - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.  \n",
    "``Image`` - a pointer to an image    \n",
    "  \n",
    "``The aim is to predic AdoptionSpeed. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: ``\n",
    "\n",
    "``0`` - Pet was adopted on the same day as it was listed.   \n",
    "``1`` - Pet was adopted between 1 and 7 days (1st week) after being listed.   \n",
    "``2`` - Pet was adopted between 8 and 30 days (1st month) after being listed.   \n",
    "``3`` - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.   \n",
    "``4`` - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).  \n",
    "  \n",
    "Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metrics exist on sklean: sklearn.metrics.cohen_kappa_score with weights=\"quadratic\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26d1bd8",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a44a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries and methods\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0d4f54",
   "metadata": {},
   "source": [
    "## Dataset Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e38d6add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the dataset they are :  9000 train observations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Description</th>\n",
       "      <th>AdoptionSpeed</th>\n",
       "      <th>Images</th>\n",
       "      <th>Breed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Dog</th>\n",
       "      <td>84.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Cream</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Small</td>\n",
       "      <td>No</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>He is either lost or abandoned. Please contact...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3b178aa59-5.jpg</td>\n",
       "      <td>Terrier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dog</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Hi, my name is Rose. I'm very friendly and am ...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2fbf2cb7c-1.jpg</td>\n",
       "      <td>Mixed_Breed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Age  Gender Color1   Color2   Color3 MaturitySize FurLength Vaccinated  \\\n",
       "Type                                                                            \n",
       "Dog   84.0    Male  Brown    Cream  Unknown        Small        No    Unknown   \n",
       "Dog    1.0  Female  Black  Unknown  Unknown       Medium       Yes         No   \n",
       "\n",
       "     Dewormed Sterilized   Health   Fee  \\\n",
       "Type                                      \n",
       "Dog       Yes         No  Healthy   0.0   \n",
       "Dog       Yes         No  Healthy  50.0   \n",
       "\n",
       "                                            Description  AdoptionSpeed  \\\n",
       "Type                                                                     \n",
       "Dog   He is either lost or abandoned. Please contact...            4.0   \n",
       "Dog   Hi, my name is Rose. I'm very friendly and am ...            3.0   \n",
       "\n",
       "               Images        Breed  \n",
       "Type                                \n",
       "Dog   3b178aa59-5.jpg      Terrier  \n",
       "Dog   2fbf2cb7c-1.jpg  Mixed_Breed  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read train files\n",
    "df_train = pd.read_csv(\"train.csv\", index_col=0)\n",
    "print(\"In the dataset they are : \", df_train.shape[0], \"train observations\")\n",
    "df_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0c527e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the dataset they are :  500 test observations\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Fee</th>\n",
       "      <th>Description</th>\n",
       "      <th>Images</th>\n",
       "      <th>Breed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Cat</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>White</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Small</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>kitten for adoption, pls call for enquiry, off...</td>\n",
       "      <td>5df99d229-2.jpg</td>\n",
       "      <td>Domestic_Short_Hair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dog</th>\n",
       "      <td>8.0</td>\n",
       "      <td>Male</td>\n",
       "      <td>Black</td>\n",
       "      <td>Brown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Stray puppy that came to my house. Obedient, w...</td>\n",
       "      <td>a08030c6f-2.jpg</td>\n",
       "      <td>Mixed_Breed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age Gender Color1 Color2   Color3 MaturitySize FurLength Vaccinated  \\\n",
       "Type                                                                        \n",
       "Cat   1.0   Male  Black  White  Unknown        Small       Yes         No   \n",
       "Dog   8.0   Male  Black  Brown  Unknown       Medium       Yes         No   \n",
       "\n",
       "     Dewormed Sterilized   Health  Fee  \\\n",
       "Type                                     \n",
       "Cat        No         No  Healthy  0.0   \n",
       "Dog        No         No  Healthy  0.0   \n",
       "\n",
       "                                            Description           Images  \\\n",
       "Type                                                                       \n",
       "Cat   kitten for adoption, pls call for enquiry, off...  5df99d229-2.jpg   \n",
       "Dog   Stray puppy that came to my house. Obedient, w...  a08030c6f-2.jpg   \n",
       "\n",
       "                    Breed  \n",
       "Type                       \n",
       "Cat   Domestic_Short_Hair  \n",
       "Dog           Mixed_Breed  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read test file\n",
    "df_test = pd.read_csv(\"test.csv\", index_col=0)\n",
    "print(\"In the dataset they are : \", df_test.shape[0], \"test observations\")\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864362a",
   "metadata": {},
   "source": [
    "# Checking for missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bdb8dd",
   "metadata": {},
   "source": [
    "I am checking for missing data because if I have the model can't learn properly, since some of the data would have values that don't represent the reality. \n",
    "Generally we use inputers to replace the missing data with statistical meassures such as the mean/median for numerical columns and the mode for categorical columns, but first we need to check if there is missing data at all.\n",
    "We will do that in both train and test datasets just in case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0dcbe2",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2444a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train dataset does not contain any missing values\n"
     ]
    }
   ],
   "source": [
    "if df_train.isnull().any().any():\n",
    "    print('There are missing values in the train dataset')\n",
    "else:\n",
    "    print('The train dataset does not contain any missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb67ba",
   "metadata": {},
   "source": [
    "``I want to be sure that there are no missing data.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9596bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9000 entries, Dog to Dog\n",
      "Data columns (total 16 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Age            9000 non-null   float64\n",
      " 1   Gender         9000 non-null   object \n",
      " 2   Color1         9000 non-null   object \n",
      " 3   Color2         9000 non-null   object \n",
      " 4   Color3         9000 non-null   object \n",
      " 5   MaturitySize   9000 non-null   object \n",
      " 6   FurLength      9000 non-null   object \n",
      " 7   Vaccinated     9000 non-null   object \n",
      " 8   Dewormed       9000 non-null   object \n",
      " 9   Sterilized     9000 non-null   object \n",
      " 10  Health         9000 non-null   object \n",
      " 11  Fee            9000 non-null   float64\n",
      " 12  Description    9000 non-null   object \n",
      " 13  AdoptionSpeed  9000 non-null   float64\n",
      " 14  Images         9000 non-null   object \n",
      " 15  Breed          9000 non-null   object \n",
      "dtypes: float64(3), object(13)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4e8a04",
   "metadata": {},
   "source": [
    "``In the cell above, I can see the types of every features and the count of non-null values for each column as we can see, there are no null values in the train dataset.``\n",
    "\n",
    "I want to do the same in the ``test`` data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5dcaa8",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d56cbf48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test dataset does not contain any missing values\n"
     ]
    }
   ],
   "source": [
    "if df_test.isnull().any().any():\n",
    "    print('There are missing values in the test dataset')\n",
    "else:\n",
    "    print('The test dataset does not contain any missing values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3c4fe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 500 entries, Cat to Dog\n",
      "Data columns (total 15 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   Age           500 non-null    float64\n",
      " 1   Gender        500 non-null    object \n",
      " 2   Color1        500 non-null    object \n",
      " 3   Color2        500 non-null    object \n",
      " 4   Color3        500 non-null    object \n",
      " 5   MaturitySize  500 non-null    object \n",
      " 6   FurLength     500 non-null    object \n",
      " 7   Vaccinated    500 non-null    object \n",
      " 8   Dewormed      500 non-null    object \n",
      " 9   Sterilized    500 non-null    object \n",
      " 10  Health        500 non-null    object \n",
      " 11  Fee           500 non-null    float64\n",
      " 12  Description   500 non-null    object \n",
      " 13  Images        500 non-null    object \n",
      " 14  Breed         500 non-null    object \n",
      "dtypes: float64(2), object(13)\n",
      "memory usage: 62.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5cb88b",
   "metadata": {},
   "source": [
    "As we can see, there are no null values in the test dataset.\n",
    "\n",
    "So we don't need to input any missing data at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350cea41",
   "metadata": {},
   "source": [
    "## Types of each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98105480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age              float64\n",
      "Gender            object\n",
      "Color1            object\n",
      "Color2            object\n",
      "Color3            object\n",
      "MaturitySize      object\n",
      "FurLength         object\n",
      "Vaccinated        object\n",
      "Dewormed          object\n",
      "Sterilized        object\n",
      "Health            object\n",
      "Fee              float64\n",
      "Description       object\n",
      "AdoptionSpeed    float64\n",
      "Images            object\n",
      "Breed             object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_train.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e359e68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age             float64\n",
      "Gender           object\n",
      "Color1           object\n",
      "Color2           object\n",
      "Color3           object\n",
      "MaturitySize     object\n",
      "FurLength        object\n",
      "Vaccinated       object\n",
      "Dewormed         object\n",
      "Sterilized       object\n",
      "Health           object\n",
      "Fee             float64\n",
      "Description      object\n",
      "Images           object\n",
      "Breed            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ffc794",
   "metadata": {},
   "source": [
    "##  Number of observations for the train and the test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8450f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have:  9000 observations in my train dataset\n",
      "I have:  500 observations in my test dataset\n"
     ]
    }
   ],
   "source": [
    "print(\"I have: \", df_train.shape[0], \"observations in my train dataset\")\n",
    "print(\"I have: \", df_test.shape[0], \"observations in my test dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf207df",
   "metadata": {},
   "source": [
    "## Separating Numerical and Categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6169cb",
   "metadata": {},
   "source": [
    "I am going to separate the numerical and categorical in different lists because later I would need to apply different trasformations to each type of column in the pipeline.\n",
    "I am not goind to take into account the ``target, description and images`` columns because they are not numerical or categorical (technically, the target is categorical but it does not matter as we want to predict it and will not use it as part of our input data). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6c4f2",
   "metadata": {},
   "source": [
    "I do not want ot change the original dataset so I will assign X (input data) and y (target) to different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd2d067",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.copy()\n",
    "X = df_train.drop('AdoptionSpeed', axis=1).copy()\n",
    "y = np.array(df_train['AdoptionSpeed']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b910f3f8",
   "metadata": {},
   "source": [
    "I will now use selectors to select the columns appropriately. The code below does the following?\n",
    "\n",
    "- Defines ``numerical_columns_selector`` which defines a column ``selector`` that selects all columns \n",
    "in X (the input data) that have a data type that is not an object (i.e., non-categorical columns).\n",
    "\n",
    "- Defines ``categorical_columns_selector`` which defines a column ``selector`` that selects all columns in X that have an object data type but excludes any columns with names that contain the strings \"Description\" or \"Images\".\n",
    "\n",
    "- Assigns to ``numerical_columns`` the result of the ``numerical_columns_selector`` applied to the dataset X, which returns a list of column names corresponding to numerical columns in X.\n",
    "\n",
    "- Assigns to ``categorical_columns`` the result of the ``categorical_columns_selector`` applied to X, which returns a list of column names corresponding to categorical columns in X that do not contain \"Description\" or \"Images\" in their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f0c785c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\compose\\_column_transformer.py:1121: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  cols = cols[cols.str.contains(self.pattern, regex=True)]\n"
     ]
    }
   ],
   "source": [
    "numerical_columns_selector = selector(dtype_exclude=object) \n",
    "categorical_columns_selector = selector(pattern=r'^(?!.*(Description|Images))',dtype_include=object)\n",
    "numerical_columns = numerical_columns_selector(X)\n",
    "categorical_columns = categorical_columns_selector(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5b4729",
   "metadata": {},
   "source": [
    "Printing the two lists, so I can verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa59d9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of numerical columns:  ['Age', 'Fee']\n",
      "List of categorical columns:  ['Gender', 'Color1', 'Color2', 'Color3', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed', 'Sterilized', 'Health', 'Breed']\n"
     ]
    }
   ],
   "source": [
    "print(\"List of numerical columns: \",numerical_columns)\n",
    "print(\"List of categorical columns: \", categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3c4577",
   "metadata": {},
   "source": [
    "### Number of observations per class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85df38c3",
   "metadata": {},
   "source": [
    "I am serching to see if the dataset is balanced or not.\n",
    "This is important because an imbalanced dataset can lead to biased models and inaccurate predictions.\n",
    "\n",
    "When a dataset is imbalanced, the model may end up being biased towards the majority class, ignoring the minority class, which can be problematic in many real-world scenarios where the minority class is of equal or even greater importance than the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fe5b6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations per class:\n",
      "  2.0    2504\n",
      "4.0    2294\n",
      "3.0    2061\n",
      "1.0    1894\n",
      "0.0     247\n",
      "Name: AdoptionSpeed, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "class_counts = df_train['AdoptionSpeed'].value_counts()\n",
    "print(\"Number of observations per class:\\n \", class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08422bff",
   "metadata": {},
   "source": [
    "I see that the class 0.0 is much less present that the others classes, that makes me think that the dataset is unbalanced. Let's see it in percentages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11ed27f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0    0.278222\n",
       "4.0    0.254889\n",
       "3.0    0.229000\n",
       "1.0    0.210444\n",
       "0.0    0.027444\n",
       "Name: AdoptionSpeed, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['AdoptionSpeed'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b904ca",
   "metadata": {},
   "source": [
    "0.0: 0.027444 (2.7%)  \n",
    "1.0: 0.210444 (21.0%)  \n",
    "2.0: 0.278222 (27.8%)  \n",
    "3.0: 0.229000 (22.9%)  \n",
    "4.0: 0.254889 (25.5%)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d7b7ed",
   "metadata": {},
   "source": [
    "We notice that the most frequent value, 2.0, appears nearly 10 times more frequently than the least frequent value, 0.0. This degree of imbalance could potentially lead to biased model performance if not handled properly. Later In the code I will use ``smote`` like we learned in the lesson how to handle with unbalance datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd243d4c",
   "metadata": {},
   "source": [
    "## Ordinal Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad5ac28",
   "metadata": {},
   "source": [
    "Ordinal encoding, is a process of transforming categorical variables into numerical variables while preserving the order or hierarchy of the categories. We can apply it to columns of our dataset which are categorical at a first glance but which some kind of progression can be identified in the unique values of the column (such as a progression in size, age, etc).\n",
    "\n",
    "To identify this, we need to study our columns' unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065ef56f",
   "metadata": {},
   "source": [
    "This function below returns a dictionary where the keys are the column names specified in columns \n",
    "and the values are the unique values in each column as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa25cc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_per_column(df, columns=[]):\n",
    "    return {col:df[col].unique().tolist() for col in (columns if columns else df)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee323aa",
   "metadata": {},
   "source": [
    "In this way I can study my data's unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9289e439",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Gender': ['Male', 'Female'],\n",
       " 'Color1': ['Brown', 'Black', 'Golden', 'White', 'Cream', 'Yellow', 'Gray'],\n",
       " 'Color2': ['Cream', 'Unknown', 'White', 'Golden', 'Gray', 'Brown', 'Yellow'],\n",
       " 'Color3': ['Unknown', 'White', 'Golden', 'Gray', 'Cream', 'Yellow'],\n",
       " 'MaturitySize': ['Small', 'Medium', 'Large', 'Extra Large'],\n",
       " 'FurLength': ['No', 'Yes', 'Unknown'],\n",
       " 'Vaccinated': ['Unknown', 'No', 'Yes'],\n",
       " 'Dewormed': ['Yes', 'No', 'Unknown'],\n",
       " 'Sterilized': ['No', 'Unknown', 'Yes'],\n",
       " 'Health': ['Healthy', 'Minor Injury', 'Serious Injury'],\n",
       " 'Breed': ['Terrier',\n",
       "  'Mixed_Breed',\n",
       "  'Cocker_Spaniel',\n",
       "  'German_Shepherd_Dog',\n",
       "  'Domestic_Medium_Hair',\n",
       "  'Bull_Terrier',\n",
       "  'Belgian_Shepherd_Malinois',\n",
       "  'English_Springer_Spaniel',\n",
       "  'Beagle',\n",
       "  'Siamese',\n",
       "  'Corgi',\n",
       "  'Siberian_Husky',\n",
       "  'Domestic_Short_Hair',\n",
       "  'Golden_Retriever',\n",
       "  'Coonhound',\n",
       "  'Maine_Coon',\n",
       "  'Persian',\n",
       "  'Tabby',\n",
       "  'Domestic_Long_Hair',\n",
       "  'Turkish_Angora',\n",
       "  'Calico',\n",
       "  'Basset_Hound',\n",
       "  'Rottweiler',\n",
       "  'English_Bulldog',\n",
       "  'Miniature_Pinscher',\n",
       "  'Ragdoll',\n",
       "  'Saint_Bernard',\n",
       "  'Labrador_Retriever',\n",
       "  'Shih_Tzu',\n",
       "  'Oriental_Tabby',\n",
       "  'Spitz',\n",
       "  'Doberman_Pinscher',\n",
       "  'Pit_Bull_Terrier',\n",
       "  'English_Cocker_Spaniel',\n",
       "  'Border_Collie',\n",
       "  'Yorkshire_Terrier_Yorkie',\n",
       "  'Singapura',\n",
       "  'West_Highland_White_Terrier_Westie',\n",
       "  'Schnauzer',\n",
       "  'Pekingese',\n",
       "  'Norwegian_Forest_Cat',\n",
       "  'Great_Dane',\n",
       "  'Bullmastiff',\n",
       "  'Tiger',\n",
       "  'Hound',\n",
       "  'Poodle',\n",
       "  'Toy_Fox_Terrier',\n",
       "  'Pomeranian',\n",
       "  'Collie',\n",
       "  'Tuxedo',\n",
       "  'Oriental_Short_Hair',\n",
       "  'Yellow_Labrador_Retriever',\n",
       "  'Bobtail',\n",
       "  'Bedlington_Terrier',\n",
       "  'Shetland_Sheepdog_Sheltie',\n",
       "  'Shar_Pei',\n",
       "  'Australian_Terrier',\n",
       "  'Jack_Russell_Terrier',\n",
       "  'Tortoiseshell',\n",
       "  'Bengal',\n",
       "  'Korat',\n",
       "  'Whippet',\n",
       "  'Dachshund',\n",
       "  'Javanese',\n",
       "  'Dalmatian',\n",
       "  'Mastiff',\n",
       "  'Manx',\n",
       "  'Abyssinian',\n",
       "  'Extra-Toes_Cat_(Hemingway_Polydactyl)',\n",
       "  'Chihuahua',\n",
       "  'American_Shorthair',\n",
       "  'British_Shorthair',\n",
       "  'Himalayan',\n",
       "  'Shepherd',\n",
       "  'Black_Labrador_Retriever',\n",
       "  'Silky_Terrier',\n",
       "  'Basenji',\n",
       "  'Burmese',\n",
       "  'Retriever',\n",
       "  'American_Water_Spaniel',\n",
       "  'American_Curl',\n",
       "  'Rat_Terrier',\n",
       "  'German_Pinscher',\n",
       "  'German_Spitz',\n",
       "  'Russian_Blue',\n",
       "  'Egyptian_Mau',\n",
       "  'Husky',\n",
       "  'Kai_Dog',\n",
       "  'Snowshoe',\n",
       "  'Oriental_Long_Hair',\n",
       "  'Belgian_Shepherd_Laekenois',\n",
       "  'Maltese',\n",
       "  'Flat-coated_Retriever',\n",
       "  'Boxer',\n",
       "  'Australian_Kelpie',\n",
       "  'Pug',\n",
       "  'Irish_Setter',\n",
       "  'Ragamuffin',\n",
       "  'Nebelung',\n",
       "  'Boston_Terrier',\n",
       "  'Munsterlander',\n",
       "  'Weimaraner',\n",
       "  'Foxhound',\n",
       "  'Jack_Russell_Terrier_(Parson_Russell_Terrier)',\n",
       "  'Shiba_Inu',\n",
       "  'Unknown',\n",
       "  'Black_Mouth_Cur',\n",
       "  'Bombay',\n",
       "  'Swedish_Vallhund',\n",
       "  'Silver',\n",
       "  'Turkish_Van',\n",
       "  'Tonkinese',\n",
       "  'American_Staffordshire_Terrier',\n",
       "  'Havana',\n",
       "  'English_Pointer',\n",
       "  'Dilute_Calico',\n",
       "  'Japanese_Bobtail',\n",
       "  'Ocicat',\n",
       "  'Greyhound',\n",
       "  'Lancashire_Heeler',\n",
       "  'Glen_of_Imaal_Terrier',\n",
       "  'Akita',\n",
       "  'Setter',\n",
       "  'Cymric',\n",
       "  'Australian_Shepherd',\n",
       "  'Wheaten_Terrier',\n",
       "  'Belgian_Shepherd_Dog_Sheepdog',\n",
       "  'Chartreux',\n",
       "  'Lhasa_Apso',\n",
       "  'Burmilla',\n",
       "  'French_Bulldog',\n",
       "  'Standard_Poodle',\n",
       "  'Cattle_Dog',\n",
       "  'Fox_Terrier',\n",
       "  'Papillon',\n",
       "  'Siberian',\n",
       "  'Welsh_Corgi',\n",
       "  'Cavalier_King_Charles_Spaniel',\n",
       "  'Birman',\n",
       "  'Somali',\n",
       "  'Lowchen',\n",
       "  'Irish_Terrier',\n",
       "  'Applehead_Siamese',\n",
       "  'Rhodesian_Ridgeback',\n",
       "  'Chinese_Crested_Dog',\n",
       "  'Manchester_Terrier',\n",
       "  'Spaniel',\n",
       "  'Chow_Chow',\n",
       "  'Samoyed',\n",
       "  'Field_Spaniel',\n",
       "  'Staffordshire_Bull_Terrier',\n",
       "  'Torbie',\n",
       "  'White_German_Shepherd']}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_per_column(df_train, columns=categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc86bf",
   "metadata": {},
   "source": [
    "The column ``MaturitySize`` is ordinal (there is a progression from small to extra large in its values), so we can map it as an ``ordinal column``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46843d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_ORD = ['MaturitySize']\n",
    "\n",
    "class ord_MAP(BaseEstimator,TransformerMixin):\n",
    "    \n",
    "    def __init__(self,cat_ORD=cat_ORD):\n",
    "        self.cat_ORD = cat_ORD\n",
    "        \n",
    "        # this maps the categorical values to numerical values across all the features\n",
    "        self.ord_map = {'Small':0, 'Medium':1, 'Large':2, 'Extra Large':3}\n",
    "    \n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        X = X.copy()\n",
    "        for att in self.cat_ORD:\n",
    "            X[att] = X[att].map(self.ord_map)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be791817",
   "metadata": {},
   "source": [
    "We need to update our lists with the categorical and numerical columns now. The ordinal column is a numerical column now since it was encoded ordinally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12dd5412",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns = numerical_columns + ['MaturitySize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ea87918",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns.remove('MaturitySize')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dccf93f",
   "metadata": {},
   "source": [
    "## Cardinality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a967f4c4",
   "metadata": {},
   "source": [
    "``Cardinality reduction`` is the process of reducing the number of unique values or levels in a categorical variable. ``Categorical`` variables can have a high number of unique values or levels and keeping those can interfere with the learning by making it take too much time and even worsening the accuracy in some cases by introducing irrelevant features with one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b703d892",
   "metadata": {},
   "source": [
    "Before we check if we need to apply it, let's take a look at the stats of our categorical columns. We exclude out text and image data as well as the target from this because there is no point to reducing the features in those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3555144c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Breed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9000</td>\n",
       "      <td>9000</td>\n",
       "      <td>9000</td>\n",
       "      <td>9000</td>\n",
       "      <td>9000</td>\n",
       "      <td>9000</td>\n",
       "      <td>9000</td>\n",
       "      <td>9000</td>\n",
       "      <td>9000</td>\n",
       "      <td>9000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Female</td>\n",
       "      <td>Black</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Healthy</td>\n",
       "      <td>Mixed_Breed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5047</td>\n",
       "      <td>4123</td>\n",
       "      <td>3078</td>\n",
       "      <td>7055</td>\n",
       "      <td>5397</td>\n",
       "      <td>4113</td>\n",
       "      <td>5710</td>\n",
       "      <td>5835</td>\n",
       "      <td>8691</td>\n",
       "      <td>3776</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Gender Color1   Color2   Color3 FurLength Vaccinated Dewormed  \\\n",
       "count     9000   9000     9000     9000      9000       9000     9000   \n",
       "unique       2      7        7        6         3          3        3   \n",
       "top     Female  Black  Unknown  Unknown       Yes        Yes      Yes   \n",
       "freq      5047   4123     3078     7055      5397       4113     5710   \n",
       "\n",
       "       Sterilized   Health        Breed  \n",
       "count        9000     9000         9000  \n",
       "unique          3        3          153  \n",
       "top            No  Healthy  Mixed_Breed  \n",
       "freq         5835     8691         3776  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train[categorical_columns].describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5973d2",
   "metadata": {},
   "source": [
    "As we can see, ost columns are fine in terms of cardinality, but we have 153 unique values for the breed column, which is a lot. We need to reduce our cardinality by grouping those values in a smaller number of categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7ac7e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceCardinalityTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, very_rare_threshold=0.001, rare_threshold=0.0025, very_uncommon_threshold=0.005, uncommon_threshold=0.01):\n",
    "        self.very_rare_threshold = very_rare_threshold\n",
    "        self.rare_threshold = rare_threshold\n",
    "        self.very_uncommon_threshold = very_uncommon_threshold\n",
    "        self.uncommon_threshold = uncommon_threshold\n",
    "        self.high_cardinality = False\n",
    "        \n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.columns_to_transform_ = X.columns[X.nunique() > 30]\n",
    "        self.mappings_ = {}\n",
    "        for col in self.columns_to_transform_:\n",
    "            value_counts = X[col].value_counts(normalize=True)\n",
    "            very_rare_values = value_counts[value_counts < self.very_rare_threshold].index.tolist()\n",
    "            rare_values = value_counts[(value_counts >= self.very_rare_threshold) & (value_counts < self.rare_threshold)].index.tolist()\n",
    "            very_uncommon_values = value_counts[(value_counts >= self.rare_threshold) & (value_counts < self.very_uncommon_threshold)].index.tolist()\n",
    "            uncommon_values = value_counts[(value_counts >= self.very_uncommon_threshold) & (value_counts < self.uncommon_threshold)].index.tolist()\n",
    "            self.mappings_[col] = {\n",
    "                'very_rare': very_rare_values,\n",
    "                'rare': rare_values,\n",
    "                'very_uncommon': very_uncommon_values,\n",
    "                'uncommon': uncommon_values\n",
    "            }\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        for col in self.columns_to_transform_:\n",
    "            mappings = self.mappings_[col]\n",
    "            X_transformed[col] = np.select(\n",
    "                [X_transformed[col].isin(mappings['very_rare']),\n",
    "                 X_transformed[col].isin(mappings['rare']),\n",
    "                 X_transformed[col].isin(mappings['very_uncommon']),\n",
    "                 X_transformed[col].isin(mappings['uncommon'])],\n",
    "                ['Very Rare', 'Rare', 'Very Uncommon', 'Uncommon'],\n",
    "                default=X_transformed[col]\n",
    "            )\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e75b69",
   "metadata": {},
   "source": [
    "The purpose of this class is to transform categorical features with high cardinality (i.e., many unique values) into categorical features with lower cardinality by ``grouping infrequent values into broader categories.``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0562488a",
   "metadata": {},
   "source": [
    "### One-Hot Encoding for Categorical Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e789e07",
   "metadata": {},
   "source": [
    "I have used OneHotEncoding to represent the categorical variable into a numerical vector (single representation)  \n",
    "``In the pipeline I will use ohe to represent the categorical variable into a numerical vectors and the standard scaler to standardise the numerical data.``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "80c044db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse=False,handle_unknown=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929adaea",
   "metadata": {},
   "source": [
    "## Text preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036f6566",
   "metadata": {},
   "source": [
    "We need to do some preprocessing with our text data. For example, it is littered with emojis and non-latin characters. Below, we write a function to clean the data of emojis, non-latin characters and other kinds of special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9879253",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee8b20",
   "metadata": {},
   "source": [
    "Those are the cleaning functions we already used in previous machine learning labs. I will be reusing them in this project in other to preprocess my text data by deleting stopwords, removing strange characters, and applying lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "241adbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_stops(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stops:\n",
    "            filtered_sentence.append(w)\n",
    "    result=' '.join(filtered_sentence)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a42033",
   "metadata": {},
   "source": [
    "Process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aea2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1(sent):\n",
    "    # sent = on sentence in a language\n",
    "    def unicode_to_ascii(s):\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    sent = unicode_to_ascii(sent.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    sent = re.sub(r\"([?.!,])\", r\" \\1 \", sent)\n",
    "    sent = re.sub(r'[\" \"]+', \" \", sent)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    sent = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sent)\n",
    "    \n",
    "    #removing emojis and non latin symbols (chinese, russian etc...)\n",
    "    emoj = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                      \"]+\", re.UNICODE)\n",
    "    sent= re.sub(emoj, '', sent)\n",
    "    \n",
    "\n",
    "    return sent.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4e0f5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def clean_text(text):\n",
    "    text=text.lower() #lowercase the text\n",
    "    #text=re.sub(r'[^\\w\\s]', '', text) #remove punctuation  Voir si besoin de a\n",
    "    text=del_stops(text) #delete stop words\n",
    "    text=lemmatizer.lemmatize(text)\n",
    "    text=step1(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f766d8",
   "metadata": {},
   "source": [
    "I preprocess the text in the 'Description' feature for both the training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "266de7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.Description = np.array([clean_text(r) for r in X.Description])\n",
    "X_test.Description = np.array([clean_text(r) for r in X_test.Description])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f558db3f",
   "metadata": {},
   "source": [
    "Here I add to the feature, \"Images\" the path to the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec22fb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X['Images']=\"train\\\\\"+ X['Images']\n",
    "X_test['Images']=\"test\\\\\"+ X_test['Images']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9adacfb",
   "metadata": {},
   "source": [
    "# Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "200a28dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install opencv-contrib-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85184c85",
   "metadata": {},
   "source": [
    "## Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d892bb4",
   "metadata": {},
   "source": [
    "SIFT (Scale-Invariant Feature Transform) is a feature extraction technique used in computer vision and image processing for detecting and describing local features in images. SIFT features are useful for a wide range of applications such as object recognition, image registration, 3D reconstruction, and image retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c45227",
   "metadata": {},
   "source": [
    "This code performs the following steps:\n",
    "\n",
    "- ``Extracts Scale-Invariant Feature Transform (SIFT)`` features from a list of images and returns a list of these features.\n",
    "\n",
    "- ``Builds a clusterizer`` (a clustering algorithm) using the list of extracted SIFT features and a desired number of clusters. The clusterizer is then fitted to the SIFT features and returned.\n",
    "\n",
    "- ``Constructs a Bag of Features (BOF)`` representation using the list of SIFT features and the fitted clusterizer. The BOF representation is a histogram of the frequency of each cluster (i.e., visual word) in the SIFT features of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60ff8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_SIFT(img_lst):\n",
    "    \n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "    sifts = []\n",
    "    \n",
    "    for img in img_lst:\n",
    "        img=cv2.imread(img)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        _, desc = sift.detectAndCompute(gray, None)\n",
    "        sifts.append(desc)\n",
    "    \n",
    "    return sifts\n",
    "\n",
    "\n",
    "\n",
    "def clusterize(SIFTs, nb_cluster):\n",
    "    \n",
    "    SIFTs = np.vstack(SIFTs)\n",
    "    clusterizer = MiniBatchKMeans(n_clusters=nb_cluster, random_state=42)\n",
    "    clusterizer.fit(SIFTs)\n",
    "    return clusterizer\n",
    "\n",
    "\n",
    "\n",
    "def build_BOFs(SIFTs, clusterizer):\n",
    "    \n",
    "    \n",
    "    BOFs = []\n",
    "    \n",
    "    for sift in SIFTs:\n",
    "        labels = clusterizer.predict(sift)\n",
    "        hist, _ = np.histogram(labels, bins=range(clusterizer.n_clusters+1), density=True)\n",
    "        BOFs.append(hist)\n",
    "        \n",
    "    return np.array(BOFs)\n",
    "\n",
    "\n",
    "class MyImageTransformer(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self, nb_cluster= 3):\n",
    "        self.nb_cluster = nb_cluster\n",
    "        self.clusterizer = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.SIFTs = extract_SIFT(X)\n",
    "        self.clusterizer = clusterize(self.SIFTs, self.nb_cluster)\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X, y=None):\n",
    "        SIFTs = extract_SIFT(X)\n",
    "        return build_BOFs(SIFTs, self.clusterizer)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.SIFTs = extract_SIFT(X)\n",
    "        self.clusterizer = clusterize(self.SIFTs, self.nb_cluster)\n",
    "        return build_BOFs(self.SIFTs, self.clusterizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609566f",
   "metadata": {},
   "source": [
    "# Pre-processing Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811580f1",
   "metadata": {},
   "source": [
    "To achieve accurate machine learning results, it's important to use a balanced dataset. This means that the number of samples for each class in the dataset should be roughly the same. If the dataset is imbalanced, the machine learning model may be biased towards the majority class, leading to poor performance on the minority class.\n",
    "\n",
    "To address this issue, my approach is to balance the dataset using a technique like SMOTE (Synthetic Minority Over-sampling Technique). SMOTE is a data augmentation technique that generates synthetic samples for the minority class by creating new samples that are combinations of existing minority samples.\n",
    "\n",
    "In this case, I plan to compare the performance of two different preprocessing approaches. The first approach will not balance the dataset, while the second approach will use SMOTE to balance the dataset. By comparing the accuracy of the two approaches, I will be able to see how much of an impact balancing the dataset has on the performance of the machine learning model.\n",
    "\n",
    "Overall, it's important to balance the dataset to avoid bias and ensure accurate machine learning results. By using a technique like SMOTE, the dataset can be balanced and the machine learning model can perform better on both majority and minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6d7c95",
   "metadata": {},
   "source": [
    "Performing ``cross-validation`` is a powerful method to evaluate the performance of a machine learning model. However, it can be computationally expensive, especially when the dataset is large. In this case, In my case I am facing a time constraint, and the computer takes a long time to perform cross-validation.\n",
    "\n",
    "To optimize my time, I am planning to only perform cross-validation on the balanced dataset since this is the recommended approach. I will not perform cross-validation on the non-balanced dataset because I only want to compare its accuracy with the accuracy of the balanced dataset. The non-balanced dataset I am using to see the difference in accuracy between the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bfd43b",
   "metadata": {},
   "source": [
    "## Building Pre-processing Pipeline (No balanced data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185cea61",
   "metadata": {},
   "source": [
    "Here I am joing every step of the preprocessing in a single pipeline.\n",
    "\n",
    "This pipeline performs a series of transformations to the raw data to prepare it for use in a machine learning model. Each step is designed to transform a specific type of data (text, image, numerical, categorical) into a format that can be used by the model. By combining these steps into a pipeline, the data can be transformed efficiently and consistently, making it easier to fit to a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7af5aa",
   "metadata": {},
   "source": [
    "- The ``general_preprocessor`` step is going to be applied only to the column MaturitySize because it is the only column with the values mapped in the transformer.\n",
    "\n",
    "- The ``text_preprocessor`` step is going to convert text into a matrix of token counts, where each row represents a document and each column represents a unique word in the dataset.\n",
    "\n",
    "- The ``image_preprocessor`` step is going to applie various image processing techniques to the images in the dataset, like I explained before, such as resizing or applying filters, to prepare them for use in a machine learning model.\n",
    "\n",
    "- The ``numerical_preprocessor`` is going with a standardization transformation to the numerical data in the dataset. Standardization scales the numerical data so that it has a mean of 0 and a standard deviation of 1, which can improve the performance of some machine learning models.\n",
    "\n",
    "- The ``categorical_preprocessor`` step  is going to reduce the cardinality of categorical features with a large number of unique values and ``ohe`` (one-hot encoding) is applied to convert the categorical features into binary vectors.\n",
    "\n",
    "- The ``Smote`` step is used to address class imbalance in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ac6c573",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_preprocessor =  ImbPipeline([\n",
    "    ('ord_map',ord_MAP())\n",
    "])\n",
    "\n",
    "text_preprocessor =  ImbPipeline([\n",
    "    ('text', CountVectorizer())\n",
    "])\n",
    "image_preprocessor =  ImbPipeline([\n",
    "    (\"img\", MyImageTransformer())\n",
    "])\n",
    "numerical_preprocessor =  ImbPipeline([\n",
    "        (\"scl\", StandardScaler())\n",
    "    ])\n",
    "categorical_preprocessor =  ImbPipeline([\n",
    "    (\"rct\", ReduceCardinalityTransformer()),\n",
    "    (\"ohe\", ohe)\n",
    "])\n",
    "\n",
    "preprocessor_without = ImbPipeline([\n",
    "    (\"ord\", ord_MAP()),\n",
    "    ('ct', ColumnTransformer([\n",
    "    ('text_preprocessor', text_preprocessor, \"Description\"),\n",
    "    ('image_preprocessor', image_preprocessor, \"Images\"),\n",
    "    ('categorical_preprocessor', categorical_preprocessor, categorical_columns),\n",
    "    ('numerical_preprocessor', numerical_preprocessor, numerical_columns)]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4dd837",
   "metadata": {},
   "source": [
    "# Building Pre-processing Pipeline (balanced dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4216e4",
   "metadata": {},
   "source": [
    "I added ``SMOTE()`` in the pipeline to balance the dataset. Like I said in one of the previous steps, balancing the dataset makes training a model more efficient because it helps prevent the model from becoming biassed towards one class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "63084f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_preprocessor =  ImbPipeline([\n",
    "    ('ord_map',ord_MAP())\n",
    "])\n",
    "\n",
    "text_preprocessor =  ImbPipeline([\n",
    "    ('text', CountVectorizer())\n",
    "])\n",
    "image_preprocessor =  ImbPipeline([\n",
    "    (\"img\", MyImageTransformer())\n",
    "])\n",
    "numerical_preprocessor =  ImbPipeline([\n",
    "        (\"scl\", StandardScaler())\n",
    "    ])\n",
    "categorical_preprocessor =  ImbPipeline([\n",
    "    (\"rct\", ReduceCardinalityTransformer()),\n",
    "    (\"ohe\", ohe)\n",
    "])\n",
    "\n",
    "\n",
    "preprocessor = ImbPipeline([\n",
    "    (\"ord\", ord_MAP()),\n",
    "    ('ct', ColumnTransformer([\n",
    "    ('text_preprocessor', text_preprocessor, \"Description\"),\n",
    "    ('image_preprocessor', image_preprocessor, \"Images\"),\n",
    "    ('categorical_preprocessor', categorical_preprocessor, categorical_columns),\n",
    "    ('numerical_preprocessor', numerical_preprocessor, numerical_columns)])),\n",
    "    ('smote', SMOTE())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c657131",
   "metadata": {},
   "source": [
    "### Splitting the Train Data into Train and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c9b34",
   "metadata": {},
   "source": [
    "I will split the train dataset in train and validation, so I can see the accuracy and confront the different models. And also because the data are 9000 \"the problem are the photos\" and it takes a loot of time every time I want to process the data and train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e29e682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.25, random_state=2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb6dc0f",
   "metadata": {},
   "source": [
    "### Fit and trasform without balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d4dad00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_without = preprocessor_without.fit_transform(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd7a06af",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_without = preprocessor_without.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bffbc6",
   "metadata": {},
   "source": [
    "### Fit and transform while balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dbb1d1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_balanced , y_train_balanced= preprocessor.fit_resample(X_train, y_train.ravel())\n",
    "X_val_balanced = preprocessor[:-1].transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b54a214",
   "metadata": {},
   "source": [
    "### The different models with the best hyper parameters, after the cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc568819",
   "metadata": {},
   "source": [
    "The RandomizedSearchCV I have done only with the balanced dataset, I haven't try even with the non balanced dataset because in my computer it takes more than one day and sometimes it froze and I needed to switch off the computer and start again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21560a9d",
   "metadata": {},
   "source": [
    "I separated with ``_without`` for the non balanced, so I don't have confusion after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a103360",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_without = LogisticRegression(solver =  'lbfgs',\n",
    "                                                penalty =  'l2',  \n",
    "                                                max_iter =  100,  \n",
    "                                                class_weight = 'balanced',\n",
    "                                                C = 0.1)\n",
    "model_svc_without = make_pipeline(svm.SVC(kernel= 'rbf', gamma= 'scale', degree= 4, class_weight= 'balanced',\n",
    "                                  C= 0.5))\n",
    "model_random_without = make_pipeline(RandomForestClassifier(n_estimators= 50, \n",
    "                                                    min_samples_leaf= 3,\n",
    "                                                    max_samples= None, \n",
    "                                                    bootstrap= False))\n",
    "model_knn_without = make_pipeline(KNeighborsClassifier(n_neighbors=5, metric='euclidean'))\n",
    "model_gbc_without = make_pipeline(GradientBoostingClassifier(n_estimators= 120, \n",
    "                                                     min_samples_split= 3,\n",
    "                                                     max_depth= 5,\n",
    "                                                     loss= 'deviance',\n",
    "                                                     learning_rate= 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bc5a09",
   "metadata": {},
   "source": [
    "##### Models where I will train with balance dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0e6f06d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic = LogisticRegression(solver =  'lbfgs',\n",
    "                                                penalty =  'l2',  \n",
    "                                                max_iter =  100,  \n",
    "                                                class_weight = 'balanced',\n",
    "                                                C = 0.1)\n",
    "model_svc = make_pipeline(svm.SVC(kernel= 'rbf', gamma= 'scale', degree= 4, class_weight= 'balanced',\n",
    "                                  C= 0.5))\n",
    "model_random = make_pipeline(RandomForestClassifier(n_estimators= 50, \n",
    "                                                    min_samples_leaf= 3,\n",
    "                                                    max_samples= None, \n",
    "                                                    bootstrap= False))\n",
    "model_knn = make_pipeline(KNeighborsClassifier(n_neighbors=5, metric='euclidean'))\n",
    "model_gbc = make_pipeline(GradientBoostingClassifier(n_estimators= 120, \n",
    "                                                     min_samples_split= 3,\n",
    "                                                     max_depth= 5,\n",
    "                                                     loss= 'deviance',\n",
    "                                                     learning_rate= 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8493f151",
   "metadata": {},
   "source": [
    "### Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76785d",
   "metadata": {},
   "source": [
    "Cross-validation with RandomizedSearchCV is a technique used to optimize the hyperparameters of a machine learning model using randomized search. RandomizedSearchCV is a function from the scikit-learn library that performs hyperparameter tuning using a combination of random search and cross-validation.\n",
    "\n",
    "The process works by defining a range of hyperparameters and their potential values. RandomizedSearchCV then randomly selects a combination of hyperparameter values from this range and trains and evaluates the model using cross-validation. The process is repeated multiple times, each time selecting a different set of hyperparameter values.\n",
    "\n",
    "The advantage of RandomizedSearchCV is that it can be more efficient than an exhaustive grid search of all possible hyperparameter combinations. By randomly sampling the hyperparameter space, RandomizedSearchCV can often find good hyperparameter values with fewer evaluations.\n",
    "\n",
    "The output of RandomizedSearchCV is the best set of hyperparameters that was found during the search, along with the corresponding cross-validation score. These hyperparameters can then be used to train the final model on the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a506a7e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'verbose', 'svc', 'svc__C', 'svc__break_ties', 'svc__cache_size', 'svc__class_weight', 'svc__coef0', 'svc__decision_function_shape', 'svc__degree', 'svc__gamma', 'svc__kernel', 'svc__max_iter', 'svc__probability', 'svc__random_state', 'svc__shrinking', 'svc__tol', 'svc__verbose'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_svc.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ddb8eb",
   "metadata": {},
   "source": [
    "I run RandomizedSearchCV for every model, to find the best hyper-parameters among the parameters I set up below. However, I am comenting it because for only one model it takes one day in my computer. I ran it only one time and I saved the best parameters to reuse later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947cd4ff",
   "metadata": {},
   "source": [
    "#### Best hyper parameters for the logistic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06b8dde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters_logistic = {}\n",
    "\n",
    "#parameters_logistic['logisticregression__penalty'] = ['l1', 'l2', 'elasticnet', 'none']\n",
    "#parameters_logistic['logisticregression__C'] = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "#parameters_logistic['logisticregression__solver'] = ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
    "#parameters_logistic['logisticregression__class_weight'] = ['dict', 'balanced', None]\n",
    "#parameters_logistic['logisticregression__max_iter'] = [80, 100, 120]\n",
    "\n",
    "#Random = RandomizedSearchCV(model_logistic, parameters_logistic, scoring='accuracy', cv=3, verbose=2,n_jobs=None)\n",
    "#Random.fit(X, y.ravel())\n",
    "#Random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3080878",
   "metadata": {},
   "source": [
    "- The result: ``{'logisticregression__solver': 'lbfgs',\n",
    " 'logisticregression__penalty': 'l2',\n",
    " 'logisticregression__max_iter': 100,\n",
    " 'logisticregression__class_weight': 'balanced',\n",
    " 'logisticregression__C': 0.1}``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e25c12",
   "metadata": {},
   "source": [
    "#### Best hyper parameters for the Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "edd154f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters_random = {}\n",
    "\n",
    "#parameters_random['randomforestclassifier__n_estimators'] = [50, 100, 150]\n",
    "#parameters_random['randomforestclassifier__min_samples_split'] = [1, 2, 3]\n",
    "#parameters_random['randomforestclassifier__min_samples_leaf'] = [1, 2, 3]\n",
    "#parameters_random['randomforestclassifier__max_samples'] = ['sqrt', 'log2', None]\n",
    "#parameters_random['randomforestclassifier__bootstrap'] = [True, False]\n",
    "\n",
    "#Random = RandomizedSearchCV(model_random, parameters_random, scoring='accuracy', cv=3, verbose=2,n_jobs=None)\n",
    "#Random.fit(X, y.ravel())\n",
    "#Random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c541929a",
   "metadata": {},
   "source": [
    "- The result: ``{'randomforestclassifier__n_estimators': 50,\n",
    " 'randomforestclassifier__min_samples_split': 2,\n",
    " 'randomforestclassifier__min_samples_leaf': 3,\n",
    " 'randomforestclassifier__max_samples': None,\n",
    " 'randomforestclassifier__bootstrap': False}``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8787cb",
   "metadata": {},
   "source": [
    "#### Best hyper parameters for the Gradient Boosting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ee5d385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters_gbc = {}\n",
    "\n",
    "#parameters_gbc['gradientboostingclassifier__n_estimators'] = [80, 100, 120]\n",
    "#parameters_gbc['gradientboostingclassifier__learning_rate'] = [0.1, 0.2, 0.3]\n",
    "#parameters_gbc['gradientboostingclassifier__max_depth'] = [3, 4, 5]\n",
    "#parameters_gbc['gradientboostingclassifier__min_samples_split'] = [2, 3, 4]\n",
    "#parameters_gbc['gradientboostingclassifier__loss'] = ['log_loss', 'deviance', 'exponential']\n",
    "\n",
    "#Random = RandomizedSearchCV(model_gbc, parameters_gbc, scoring='accuracy', cv=3, verbose=2,n_jobs=None)\n",
    "#Random.fit(X, y.ravel())\n",
    "#Random.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad42ca7f",
   "metadata": {},
   "source": [
    "- The Result: ``{'gradientboostingclassifier__n_estimators': 120,\n",
    " 'gradientboostingclassifier__min_samples_split': 3,\n",
    " 'gradientboostingclassifier__max_depth': 5,\n",
    " 'gradientboostingclassifier__loss': 'deviance',\n",
    " 'gradientboostingclassifier__learning_rate': 0.2}``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8656cbb",
   "metadata": {},
   "source": [
    "#### Best hyper parameters for the SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b17768c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters_svc = {}\n",
    "\n",
    "#parameters_svc['svc__C'] = [0.5, 1.0, 1.5]\n",
    "#parameters_svc['svc__gamma'] = ['scale', 'auto']\n",
    "#parameters_svc['svc__kernel'] = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "#parameters_svc['svc__degree'] = [2, 3, 4]\n",
    "#parameters_svc['svc__class_weight'] = ['dict', 'balanced', None]\n",
    "\n",
    "#Random = RandomizedSearchCV(model_svc, parameters_svc, scoring='accuracy', cv=3, verbose=2,n_jobs=None)\n",
    "#Random.fit(X, y.ravel())\n",
    "#print(\"the best parameters: \", Random.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d91d4",
   "metadata": {},
   "source": [
    "- The result:  {'svc__kernel': 'rbf', 'svc__gamma': 'scale', 'svc__degree': 4, 'svc__class_weight': 'balanced', 'svc__C': 0.5}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b129eb4f",
   "metadata": {},
   "source": [
    "## Train the models (not balance dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85b8a734",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;gradientboostingclassifier&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, loss=&#x27;deviance&#x27;,\n",
       "                                            max_depth=5, min_samples_split=3,\n",
       "                                            n_estimators=120))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;gradientboostingclassifier&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, loss=&#x27;deviance&#x27;,\n",
       "                                            max_depth=5, min_samples_split=3,\n",
       "                                            n_estimators=120))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.2, loss=&#x27;deviance&#x27;, max_depth=5,\n",
       "                           min_samples_split=3, n_estimators=120)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('gradientboostingclassifier',\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, loss='deviance',\n",
       "                                            max_depth=5, min_samples_split=3,\n",
       "                                            n_estimators=120))])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistic_without.fit(X_train_without , y_train.ravel())\n",
    "model_random_without.fit(X_train_without , y_train.ravel()) \n",
    "model_svc_without.fit(X_train_without , y_train.ravel())\n",
    "model_knn_without.fit(X_train_without , y_train.ravel())\n",
    "model_gbc_without.fit(X_train_without , y_train.ravel())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3028a9",
   "metadata": {},
   "source": [
    "## Train the models (balance dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c82785da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;gradientboostingclassifier&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, loss=&#x27;deviance&#x27;,\n",
       "                                            max_depth=5, min_samples_split=3,\n",
       "                                            n_estimators=120))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;gradientboostingclassifier&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, loss=&#x27;deviance&#x27;,\n",
       "                                            max_depth=5, min_samples_split=3,\n",
       "                                            n_estimators=120))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.2, loss=&#x27;deviance&#x27;, max_depth=5,\n",
       "                           min_samples_split=3, n_estimators=120)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('gradientboostingclassifier',\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, loss='deviance',\n",
       "                                            max_depth=5, min_samples_split=3,\n",
       "                                            n_estimators=120))])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistic.fit(X_train_balanced , y_train_balanced.ravel())\n",
    "model_random.fit(X_train_balanced , y_train_balanced.ravel())\n",
    "model_svc.fit(X_train_balanced , y_train_balanced.ravel())\n",
    "model_knn.fit(X_train_balanced , y_train_balanced.ravel())\n",
    "model_gbc.fit(X_train_balanced , y_train_balanced.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8ee4bc",
   "metadata": {},
   "source": [
    "### Predict (Not balance dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37558592",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_without_logistic = model_logistic_without.predict(X_val_without)\n",
    "y_pred_without_random = model_random_without.predict(X_val_without)\n",
    "y_pred_without_svc = model_svc_without.predict(X_val_without)\n",
    "y_pred_without_knn = model_knn_without.predict(X_val_without)\n",
    "y_pred_without_gbc = model_gbc_without.predict(X_val_without)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9e7317",
   "metadata": {},
   "source": [
    "### Predict (balance dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f39192d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic = model_logistic.predict(X_val_balanced)\n",
    "y_pred_random = model_random.predict(X_val_balanced)\n",
    "y_pred_svc = model_svc.predict(X_val_balanced)\n",
    "y_pred_knn = model_knn.predict(X_val_balanced)\n",
    "y_pred_gbc = model_gbc.predict(X_val_balanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322471d",
   "metadata": {},
   "source": [
    "## The metrics of the different models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b1bcb",
   "metadata": {},
   "source": [
    "As explicited before, we use ``cohen_kappa_score`` to assess our model's metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b736c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: unbalance dataset\n",
      "logistic:  0.19412174801377768\n",
      "random:  0.22065256719400583\n",
      "svc:  0.18797176208782884\n",
      "knn:  0.16875614274791373\n",
      "gbc:  0.22351317151731276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "kappa_train_logistic_without =cohen_kappa_score(y_valid, y_pred_without_logistic)\n",
    "kappa_train_random_without =cohen_kappa_score(y_valid, y_pred_without_random)\n",
    "kappa_train_svc_without =cohen_kappa_score(y_valid, y_pred_without_svc)\n",
    "kappa_train_knn_without =cohen_kappa_score(y_valid, y_pred_without_knn)\n",
    "kappa_train_gbc_without =cohen_kappa_score(y_valid, y_pred_without_gbc)\n",
    "\n",
    "print(\"Kappa Score: unbalance dataset\")\n",
    "print(\"logistic: \", kappa_train_logistic_without)\n",
    "print(\"random: \", kappa_train_random_without)\n",
    "print(\"svc: \", kappa_train_svc_without)\n",
    "print(\"knn: \", kappa_train_knn_without)\n",
    "print(\"gbc: \", kappa_train_gbc_without)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "540728e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kappa Score: balance dataset\n",
      "logistic:  0.19679139534196333\n",
      "random:  0.21684547326832548\n",
      "svc:  0.16178027652737104\n",
      "knn:  0.12546240647155404\n",
      "gbc:  0.24697195383631187\n"
     ]
    }
   ],
   "source": [
    "kappa_train_logistic =cohen_kappa_score(y_valid, y_pred_logistic)\n",
    "kappa_train_random =cohen_kappa_score(y_valid, y_pred_random)\n",
    "kappa_train_svc =cohen_kappa_score(y_valid, y_pred_svc)\n",
    "kappa_train_knn =cohen_kappa_score(y_valid, y_pred_knn)\n",
    "kappa_train_gbc =cohen_kappa_score(y_valid, y_pred_gbc)\n",
    "\n",
    "print(\"Kappa Score: balance dataset\")\n",
    "print(\"logistic: \", kappa_train_logistic)\n",
    "print(\"random: \", kappa_train_random)\n",
    "print(\"svc: \", kappa_train_svc)\n",
    "print(\"knn: \", kappa_train_knn)\n",
    "print(\"gbc: \", kappa_train_gbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fedc1a",
   "metadata": {},
   "source": [
    "Between the two methods I don't see a big difference in the accuracy, maybe it is because I am not using the true labels for comparing and I am not using the actual test data to predict but rather cross validation, which while useful, does not necessarily reflect the real data. From my experiment I did I don't see an improvement in the accuracy balancing the data, some models are even worse with balancing the data. But the right thing to do in our case is balancing the data as we have one class that appears very little."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa28b06",
   "metadata": {},
   "source": [
    "## Using the full data set for training and creating the predictions for the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b7d42",
   "metadata": {},
   "source": [
    "We can now train a model with our full dataset by using the best parameters we found before, and then use it on our test data to generate the values we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "93c3dbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_true , y_true= preprocessor.fit_resample(X, y.ravel())\n",
    "X_test_true = preprocessor[:-1].transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7508aadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_logistic_true = LogisticRegression(solver =  'lbfgs',\n",
    "                                                penalty =  'l2',  \n",
    "                                                max_iter =  100,  \n",
    "                                                class_weight = 'balanced',\n",
    "                                                C = 0.1)\n",
    "model_svc_true = make_pipeline(svm.SVC(kernel= 'rbf', gamma= 'scale', degree= 4, class_weight= 'balanced',\n",
    "                                  C= 0.5))\n",
    "model_random_true = make_pipeline(RandomForestClassifier(n_estimators= 50, \n",
    "                                                    min_samples_leaf= 3,\n",
    "                                                    max_samples= None, \n",
    "                                                    bootstrap= False))\n",
    "model_knn_true = make_pipeline(KNeighborsClassifier(n_neighbors=5, metric='euclidean'))\n",
    "model_gbc_true = make_pipeline(GradientBoostingClassifier(n_estimators= 120, \n",
    "                                                     min_samples_split= 3,\n",
    "                                                     max_depth= 5,\n",
    "                                                     loss= 'deviance',\n",
    "                                                     learning_rate= 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "165914b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\Raffaele\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py:280: FutureWarning: The loss parameter name 'deviance' was deprecated in v1.1 and will be removed in version 1.3. Use the new parameter name 'log_loss' which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;gradientboostingclassifier&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, loss=&#x27;deviance&#x27;,\n",
       "                                            max_depth=5, min_samples_split=3,\n",
       "                                            n_estimators=120))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;gradientboostingclassifier&#x27;,\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, loss=&#x27;deviance&#x27;,\n",
       "                                            max_depth=5, min_samples_split=3,\n",
       "                                            n_estimators=120))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.2, loss=&#x27;deviance&#x27;, max_depth=5,\n",
       "                           min_samples_split=3, n_estimators=120)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('gradientboostingclassifier',\n",
       "                 GradientBoostingClassifier(learning_rate=0.2, loss='deviance',\n",
       "                                            max_depth=5, min_samples_split=3,\n",
       "                                            n_estimators=120))])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_logistic_true.fit(X_true , y_true.ravel())\n",
    "model_random_true.fit(X_true , y_true.ravel()) \n",
    "model_svc_true.fit(X_true , y_true.ravel())\n",
    "model_knn_true.fit(X_true , y_true.ravel())\n",
    "model_gbc_true.fit(X_true , y_true.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7042398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic_true = model_logistic_true.predict(X_test_true)\n",
    "y_pred_random_true = model_random_true.predict(X_test_true)\n",
    "y_pred_svc_true = model_svc_true.predict(X_test_true)\n",
    "y_pred_knn_true = model_knn_true.predict(X_test_true)\n",
    "y_pred_gbc_true = model_gbc_true.predict(X_test_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213f3634",
   "metadata": {},
   "source": [
    "Like I said before, I had the best accuracy with Gradient Boosting Classifier, I choose it as the best model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca1531",
   "metadata": {},
   "source": [
    "__A little explanation about Gradient Boosting__\n",
    "\n",
    "Gradient Boosting is an ensemble method that combines multiple decision trees to improve the predictive performance of a model.\n",
    "\n",
    "In Gradient Boosting, decision trees are built sequentially, with each tree trying to correct the errors made by the previous tree. The algorithm learns by minimizing a loss function, such as binary cross-entropy for classification problems, using gradient descent optimization. The final prediction is obtained by taking a weighted average of the predictions from all the trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "974c2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_pred = {}\n",
    "models_pred['logistic'] = y_pred_logistic_true\n",
    "models_pred['random'] = y_pred_random_true\n",
    "models_pred['svc'] = y_pred_svc_true\n",
    "models_pred['knn'] = y_pred_knn_true\n",
    "models_pred['gbc'] = y_pred_gbc_true\n",
    "models_pred['best_gbc'] = y_pred_gbc_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b38acd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     logistic  random  svc  knn  gbc  best_gbc\n",
      "0         1.0     1.0  1.0  1.0  1.0       1.0\n",
      "1         2.0     2.0  1.0  1.0  4.0       4.0\n",
      "2         2.0     2.0  0.0  0.0  2.0       2.0\n",
      "3         4.0     2.0  3.0  1.0  2.0       2.0\n",
      "4         0.0     1.0  0.0  0.0  2.0       2.0\n",
      "..        ...     ...  ...  ...  ...       ...\n",
      "495       4.0     4.0  1.0  0.0  1.0       1.0\n",
      "496       1.0     1.0  1.0  1.0  1.0       1.0\n",
      "497       4.0     4.0  4.0  4.0  4.0       4.0\n",
      "498       3.0     4.0  4.0  4.0  4.0       4.0\n",
      "499       4.0     3.0  0.0  0.0  2.0       2.0\n",
      "\n",
      "[500 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(models_pred)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c2ea3278",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
