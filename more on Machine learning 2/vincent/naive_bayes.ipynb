{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ae4d087-0984-478c-8761-103f42756a01",
   "metadata": {},
   "source": [
    "# The naive Bayes classifier\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A supervised classification method, with a lot of assumptions ..., but well adapted to high dimensional data with moderate sample size. \n",
    "\n",
    "As reminder:\n",
    "- logistic regression puts a model on $P(y|x)$ and estimates the parameters by minimizing the cross-entropy loss\n",
    "- SVM puts a model only on the classification boundary and research the separating hyperplane with largest margin \n",
    "\n",
    "Another philosophy is to consider a generative model on data distribution, i.e. model $P(x,y)$ by writting $P(x,y) = P(y)P(x|y)$. Thus it is only needed to estimate $P(x|y)$ for each possible value of $y$ (multivariate gaussian distribution, multivariate multinomial distribution, ...), and also $P(y)$ the prior probabilities (typically simply the proportion of each class). Then the posterior probability $P(y|x)$ can be computed by using the **Bayes theorem**: \n",
    "\n",
    "$$\n",
    "P(y|x) = \\frac{P(y)P(x|y)}{P(x)} = \\frac{P(y)P(x|y)}{\\sum_{k=1}^{K} P(y=k) P(x|y=k)}\n",
    "$$\n",
    "\n",
    "\n",
    "## The model\n",
    "\n",
    "Let assume that $x \\in \\mathcal{X}$ : \n",
    "- **Continuous features**: $\\mathcal{X}=\\mathcal{R}^d$, $P(x|y=k)$ is modeled by a multivariate density.\n",
    "- **Categorical features**: $\\mathcal{X}=\\prod_{j=1}^{d}\\{1, ..., m_j\\}$, $P(x|y=k)$ is modeled by a multivariate probability.\n",
    "- Also possible **mixed data**: both continuous and categorical\n",
    "\n",
    "**Question**: What model use for $P(x|y=k)$?\n",
    "\n",
    "**Some ideas**:\n",
    "- Multivariate normal distribution (need only to estimate the vector of expectations and the variance covariance matrix) \n",
    "- Mixture of multivariate normal distribution for more flexibility\n",
    "- Not so many models for categorical data ... \n",
    "\n",
    "**The Naives assumtion**:\n",
    "$$\n",
    "P(x|y=k) = P(x_1, \\ldots, x_d |y=k) = \\prod_{j=1}^{d} P(x_j|y=k),\n",
    "$$ \n",
    "\n",
    "It assumes that all the variables are independant given the class. \n",
    "\n",
    "- Avoid to model dependency! \n",
    "- Only need to model $P(x_j|y=k)$ by a univariate distribution\n",
    "    - univariate discrete probability distribution for categorical variables (multinomial, Poisson, ...) \n",
    "    - univariate probability density function (normal, Student, Gamma, exponential, ...)\n",
    "\n",
    "\n",
    "Let assume that $P(x_j|y=k)$ belong to some parametric family we will denote  $P(x_j|y=k) := P(x_j|\\theta_{kj})$  where $\\theta_{kj}$ are the paramters of the distribution of variable $j$ in class $k$:\n",
    "- $\\theta_{kj} = (\\mu_{kj} , \\sigma_{kj}^2)$ for a Gaussian distribution\n",
    "- $\\theta_{kj} = (\\alpha_{kj1}, ..., \\alpha_{kjm_{j}})$ for a multinomial distribution where $\\alpha_{kjh}$ is the probability of the model $h$ of variable $j$ in class $k$\n",
    "\n",
    "\n",
    "## Parameters estimation\n",
    "\n",
    "Parameters are estimated by maximum likelihood as in statistical inference!\n",
    "\n",
    "Let $\\theta_k = (\\theta_{k1}, ..., \\theta_{kd})$ that groups all the parameter of class $k$, let also denote by $\\pi_k = P(y=k)$ the prior probability of class which also need to be estimated. \n",
    "\n",
    "Let denote by $\\theta = (\\pi_1, ..., \\pi_K, \\theta_1, \\ldots, \\theta_K)$ that groups all the paramters of the model \n",
    "\n",
    "The likelihood is: \n",
    "$$\n",
    "\\ell(\\theta) = \\sum_{k=1}^{K} \\sum_{i \\in \\mathcal{C}_k} \\pi_k \\log P(x_i | \\theta_k)\n",
    "$$\n",
    "with $\\mathcal{C}_k$ the set of data points belonging to class $k$\n",
    "\n",
    "\n",
    "**Estimation of the prior probabilities**\n",
    "\n",
    "For all $k \\in \\{1, ..., K \\}$\n",
    "$$\n",
    "\\hat \\pi_k = \\frac{n_k}{n}\n",
    "$$\n",
    "with $n_k$ the number of data in class $k$, among the total of $n$ data. \n",
    "\n",
    "*Remark*: This can depend on the sampling scheme, for instance in the medical setting we can consider restrospective sampling, i.e. fix by advance the proportion of each class in the training data, for instance for rare diseases have the same number of patient with and without the disease in the cohort. In this case the user can give manually the proportion of each class in the whole population.  \n",
    "\n",
    "\n",
    "**Gaussian distribution**\n",
    "For $k \\in \\{1, ..., K\\}$, $j \\in \\{1,...,d\\}$\n",
    "$$\n",
    "\\hat\\mu_{kj} = \\frac{\\sum_{i \\in \\mathcal{C}_k} x_{ij}}{n_k}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\hat\\sigma_{kj}^2 = \\frac{\\sum_{i \\in \\mathcal{C}_k} (x_{ij}-\\hat\\mu_{kj})^2}{n_k}\n",
    "$$\n",
    "\n",
    "\n",
    "**Multinomial distribution**\n",
    "For $k \\in \\{1, ..., K\\}$, $j \\in \\{1,...,d\\}$ and $h \\in \\{1,...,m_j\\}$\n",
    "$$\n",
    "\\hat\\alpha_{kjh} = \\frac{n_{kjh}}{n_k}\n",
    "$$\n",
    "with $n_{kjh}$ the number of times that modality $h$ of variable $j$ has been observed in class $k$.\n",
    "\n",
    "*Remark*: Regularized version of the estimators can be consider to avoid to estimate some $\\hat\\alpha_{kjh}$ to $0$. For instance $\\hat\\alpha_{kjh} = \\frac{n_{kjh} + c}{n_k + c m_j}$ where $c>0$ is some regularization parameter.\n",
    "\n",
    "\n",
    "## Prediction of the class for new data\n",
    "\n",
    "Based on $\\hat\\theta$, it is possible to compute $\\forall k \\in \\{1, ..., K\\}$:\n",
    "$$\n",
    "P(y=k|x, \\hat\\theta) = \\frac{\\hat\\pi_k P(x|\\hat\\theta_k)}{\\sum_{k=1}^{K} \\hat\\pi_k P(x|\\hat\\theta_k)}\n",
    "$$\n",
    "\n",
    "Then the predicted class can be obtained by Maximum A Posteriori (MAP)\n",
    "\n",
    "$$\n",
    "\\hat y = \\arg\\max_{k \\in \\{1, ..., K \\}} P(y=k|x, \\hat\\theta)\n",
    "$$\n",
    "\n",
    " \n",
    "## Discussion \n",
    "\n",
    "The model make stong assumptions, i.e. models the distribution of $(x,y)$, where only the distribution of $y|x$ or even only the decision boundary is needed to make prediction. \n",
    "\n",
    "Thus this leds to model miss-specification which may degrade the performance of the final classifier. However such kind of model can still work well with moderate sample size and high number of variables. See for instance [1]\n",
    "\n",
    "[1] Hand, D. J., & Yu, K. (2001). Idiot's Bayesâ€”not so stupid after all?. International statistical review, 69(3), 385-398.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8157aa0a-fe23-4342-9289-30959038b09f",
   "metadata": {},
   "source": [
    "# 1. Naive Bayes in sklearn\n",
    "\n",
    "More information can be found on :\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "The main functions are :\n",
    "- `GaussianNB` : Naive Bayes for continuous features using Gaussian assumption\n",
    "- `BernoulliNB` : Naive Bayes classifier for multivariate Bernoulli models.\n",
    "- `CategoricalNB` : Naive Bayes classifier for categorical features.\n",
    "- `MultinomialNB` : Naive Bayes classifier for multinomial models.\n",
    "- `ComplementNB` : Complement Naive Bayes classifier.\n",
    "\n",
    "For mixed-type of features there is not dedicated function in sklearn, but continous features can for instances be discretized in order to use `ComplementNB` on the discretized dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f6246c-756e-4649-aec9-80f1db8300ad",
   "metadata": {},
   "source": [
    "**Q1** : By looking at the documentation explain the differences between `BernoulliNB`, `CategoricalNB` and `MultinomialNB`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ab1550-e29a-4edd-a5e0-e77d4e7ef624",
   "metadata": {},
   "source": [
    "## Continuous features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46396d53-2c74-4fcb-b51f-a4498c7d4c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#necessary imports\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from random import randint\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, ConfusionMatrixDisplay\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5f96d1-a408-4df4-8b21-6327867fa948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90cffdba-d34b-4f70-83e6-c4e6c5edb0bd",
   "metadata": {},
   "source": [
    "**Q2**: Apply the `GaussianNB` to train the model on the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc2c87f-1a21-4509-a97b-fa97087ea372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1b32379f-f01d-45ab-b278-f6f926041b01",
   "metadata": {},
   "source": [
    "**Q3**: Let consider the following functions\n",
    "- `predict_joint_log_proba(X)`\n",
    "- `predict_log_proba(X)`\n",
    "- `predict_proba(X)`\n",
    "\n",
    "(a) Explain what the above functions do and illustrate it on the iris dataset\n",
    "\n",
    "(b) Explain the use of making use of the log instead of directly computing the probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393c59fd-0c0b-464b-97eb-47f308194cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3824b6e1-8179-489b-8e27-1b8367bf1ac6",
   "metadata": {},
   "source": [
    "**Q3bis** : Have a look at some basic implementation of the Naive Bayes algorithm \n",
    "\n",
    "https://www.python-engineer.com/courses/mlfromscratch/05_naivebayes/\n",
    "\n",
    "(a) Complete the code in order to compute the posterior membership probabilities (and not only the class with the highest posterior probability)\n",
    "\n",
    "(b) Also consider directly compute the log of the pdf rather than the pdf for numerical stability\n",
    "\n",
    "(c) What modification should you consider to consider categorical features?\n",
    "\n",
    "(d) Give some ideas of how it would be possible to consider mixed type of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8612594-df92-46f3-9dab-0c32bcd2d9fb",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e7066a-8cfc-49c0-860a-e3ffeb408543",
   "metadata": {},
   "source": [
    "**Q4**: Use `KBinsDiscretizer` of `sklearn.preprocessing` in order to discrtize the iris data with `n_bins=2`, then train a naive bayes classifier in this discretized dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb503b9d-2c6b-481d-8676-9e38699eb9c1",
   "metadata": {},
   "source": [
    "**Q5**: Idem but with `n_bins = 3`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f103385-22fc-4b69-ac8f-ee77fa9ffe6e",
   "metadata": {},
   "source": [
    "**Q6**: Compare the performances of the different approches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57b3e2-d5ee-4999-9153-a719af33a59c",
   "metadata": {},
   "source": [
    "## Application on the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc362d6-b032-48ca-9d81-4e9532f6d666",
   "metadata": {},
   "source": [
    "**Q7**: Apply the Naive Bayes classifier on the MNIST dataset and comment its performances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4517cab-2b5d-413b-9e92-a2854d010e9b",
   "metadata": {},
   "source": [
    "## 2. Linear and quadratic discriminant analysis\n",
    "\n",
    "The model now considered is not the naives Bayes, since it does not any more the assumption of conditional independence. It is however still a generative classifier since it models the distribution of $(x,y)$ then it deduce the distribution of $y|x$ using the Bayes theorem. \n",
    "\n",
    "More information can be found on : \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html\n",
    "\n",
    "The **linear discriminant analysis** assumes multivariate Gaussian distribution in each class, and moreover the covariance matrices are assumed to be the same in each class: \n",
    "$$\n",
    "x | y = k \\sim \\mathcal{N}(\\mu_k , \\Sigma)\n",
    "$$\n",
    "This results in a linear decision boundary. \n",
    "\n",
    "\n",
    "The **quadratic discriminant analysis** assumes multivariate Gaussian distribution in each class, and moreover the covariance matrices are assumed to be the same in each class: \n",
    "$$\n",
    "x | y = k \\sim \\mathcal{N}(\\mu_k , \\Sigma_k).\n",
    "$$\n",
    "This results in a quadratic decision boundary.\n",
    "\n",
    "**Remarks** : \n",
    "\n",
    "- Linear discriminant analysis can also be used for dimension reduction by looking for the most discriminant components (K-1 discriminant components if $K$ classes are considered) \n",
    "- In high dimensional setting the `shrinkage` parameter can be used in order to regularize the estimator of the covariance matrix, for example of the kind $(1-\\alpha)\\hat\\Sigma + \\alpha I$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df583830-fc15-4a9e-baf7-e96389fadf49",
   "metadata": {},
   "source": [
    "**Q8** : Fit a linear discriminant analysis on the MNIST dataset, try to optimize the value of the shrinkage parameter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
