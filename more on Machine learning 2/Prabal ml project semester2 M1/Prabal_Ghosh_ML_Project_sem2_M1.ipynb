{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;color: blue\"> Pet Finder using Machine Learning </p>\n",
    "### <p style=\"text-align: center;color: green\"> Prabal Ghosh </p>\n",
    "\n",
    "#### <p style=\"text-align: center;\"> MSc Data Science & Artificial Intelligence </p>\n",
    "#### <p style=\"text-align: center;\"> More on Learning Algorithms </p> \n",
    "#### <p style=\"text-align: center;\"> 15/03/2024 </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The dataset presents pet's characteristics and includes tabular, text and image data.\n",
    "``The aim is to predict the rate at which a pet is adopted.``\n",
    "\n",
    "Data fields:\n",
    "\n",
    "``Type`` - Dog or Cat  \n",
    "``Age`` - Age of pet when listed, in months  \n",
    "``Gender`` - Gender of pet (Male, Female, Mixed, if profile represents group of pets)  \n",
    "``Color1`` - Color 1 of pet  \n",
    "``Color2`` - Color 2 of pet   \n",
    "``Color3`` - Color 3 of pet   \n",
    "``MaturitySize`` - Size at maturity (Small, Medium, Large, Extra Large, Not Specified)  \n",
    "``FurLength`` - Fur length (Short, Medium, Long, Not Specified)  \n",
    "``Vaccinated`` - Pet has been vaccinated (Yes, No, Not Sure)  \n",
    "``Dewormed`` - Pet has been dewormed (Yes, No, Not Sure)  \n",
    "``Sterilized`` - Pet has been spayed / neutered (Yes, No, Not Sure)  \n",
    "``Health`` - Health Condition (Healthy, Minor Injury, Serious Injury, Not Specified)  \n",
    "``Fee`` - Adoption fee (0 = Free)  \n",
    "``Breed`` - breed of pet (see on the dataset)  \n",
    "``Description`` - Profile write-up for this pet. The primary language used is English, with some in Malay or Chinese.  \n",
    "``Image`` - a pointer to an image    \n",
    "  \n",
    "``The aim is to predic AdoptionSpeed. The value is determined by how quickly, if at all, a pet is adopted. The values are determined in the following way: ``\n",
    "\n",
    "``0`` - Pet was adopted on the same day as it was listed.   \n",
    "``1`` - Pet was adopted between 1 and 7 days (1st week) after being listed.   \n",
    "``2`` - Pet was adopted between 8 and 30 days (1st month) after being listed.   \n",
    "``3`` - Pet was adopted between 31 and 90 days (2nd & 3rd month) after being listed.   \n",
    "``4`` - No adoption after 100 days of being listed. (There are no pets in this dataset that waited between 90 and 100 days).  \n",
    "  \n",
    "Submissions are scored based on the quadratic weighted kappa, which measures the agreement between two ratings. This metrics exist on sklean: sklearn.metrics.cohen_kappa_score with weights=\"quadratic\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline- https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "- KNN- https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "- RandomOverSampler- https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html\n",
    "- RandomizedSearchCV- https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html\n",
    "- https://scikit-learn.org/stable/modules/compose.html\n",
    "- https://towardsdatascience.com/pipelines-custom-transformers-in-scikit-learn-the-step-by-step-guide-with-python-code-4a7d9b068156\n",
    "- https://towardsdatascience.com/customizing-scikit-learn-pipelines-write-your-own-transformer-fdaaefc5e5d7\n",
    "-  https://www.youtube.com/watch?v=aijB8qbEOQ4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries and methods\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.compose import make_column_selector as selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import set_config\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "import unicodedata\n",
    "\n",
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "#packages\n",
    "#basics\n",
    "import os\n",
    "\n",
    "\n",
    "#modeling\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "#visualizations\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Import necessary  Libraries\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install opencv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train files\n",
    "df_train = pd.read_csv(\"C:\\\\Users\\\\praba\\\\Documents\\\\GitHub\\\\UCA SEMESTER 2 M1\\\\deeplearning 2\\\\final project and lab\\\\drive-download-20240301T144626Z-001\\\\train.csv\")\n",
    "print(\"In the dataset they are : \", df_train.shape[0], \"train observations\")\n",
    "# df_train.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read test file\n",
    "df_test = pd.read_csv(\"C:\\\\Users\\\\praba\\\\Documents\\\\GitHub\\\\UCA SEMESTER 2 M1\\\\deeplearning 2\\\\final project and lab\\\\drive-download-20240301T144626Z-001\\\\test.csv\")\n",
    "print(\"In the dataset they are : \", df_test.shape[0], \"test observations\")\n",
    "# df_test.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy = df_train.copy()\n",
    "df_train_copy.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy = df_test.copy()\n",
    "df_test_copy.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  image path add for train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"C:\\\\Users\\\\praba\\\\Documents\\\\GitHub\\\\UCA SEMESTER 2 M1\\\\deeplearning 2\\\\final project and lab\\\\drive-download-20240301T144626Z-001\\\\train_images_all\\\\\"\n",
    "test_img_dir = \"C:\\\\Users\\\\praba\\\\Documents\\\\GitHub\\\\UCA SEMESTER 2 M1\\\\deeplearning 2\\\\final project and lab\\\\drive-download-20240301T144626Z-001\\\\test_images_all\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_train_copy['Images'] = [img_dir+img for img in df_train_copy['Images']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy['Images'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy['Images'] = [test_img_dir+img for img in df_test_copy['Images']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy['Images'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "- **No NAN value present.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_series = df_train_copy['AdoptionSpeed'].value_counts().sort_index()\n",
    "print(ordered_series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Color3 Feature has 7055 Unknown values so drop this cloum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy.Color3.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Color3 will be dropped as 7055 data values are unknown out of 9000**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Type and AdoptionSpeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display species graph\n",
    "plt.subplot(1, 2, 1)\n",
    "df_train_copy['Type'].value_counts().plot(kind='bar', figsize=(12,5),color=['blue','green'])\n",
    "\n",
    "plt.title('Species')\n",
    "plt.xlabel('Count')\n",
    "\n",
    "# display adoption speed graph\n",
    "plt.subplot(1, 2, 2)\n",
    "df_train_copy['AdoptionSpeed'].value_counts().sort_index().plot(kind='bar',color = [\"red\",\"yellow\",'blue','green','orange'])\n",
    "plt.title('Adoption Speed')\n",
    "plt.xlabel('Count')\n",
    "print(df_train_copy['AdoptionSpeed'].value_counts())\n",
    "\n",
    "# minimize overlap\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/python-plotting-basics-simple-charts-with-matplotlib-seaborn-and-plotly-e36346952a3a\n",
    "fig, ax = plt.subplots(figsize=(3,4))\n",
    "plt.rcParams['font.size']=17\n",
    "#percent count\n",
    "labels = [\"dog\",\"cat\"]\n",
    "percentages = [(df_train_copy[df_train_copy[\"Type\"]==\"Dog\"].AdoptionSpeed.shape[0]*100)/df_train_copy.shape[0],\n",
    "               (df_train_copy[df_train_copy[\"Type\"]==\"Cat\"].AdoptionSpeed.shape[0]*100)/df_train_copy.shape[0]]\n",
    "\n",
    "ax.pie(percentages, labels=labels,  \n",
    "        autopct='%1.0f%%', \n",
    "       shadow=False, startangle=0,   \n",
    "       pctdistance=0.6,labeldistance=0.8)\n",
    "ax.axis('equal')\n",
    "ax.set_title(\"Cat and Dog distribution\")\n",
    "ax.legend(frameon=False, bbox_to_anchor=(1.5,0.8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of dog pet is higher than the number of cat in the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdoptionSpeed ( Target   **Unbalanced**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count observations per class\n",
    "class_counts = df_train_copy['AdoptionSpeed'].value_counts()\n",
    "\n",
    "# Display results\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://towardsdatascience.com/python-plotting-basics-simple-charts-with-matplotlib-seaborn-and-plotly-e36346952a3a\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "plt.rcParams['font.size']=17\n",
    "#percent count\n",
    "labels = [\"0\",\"1\", \"2\", \"3\", \"4\"]\n",
    "percentages = [(df_train_copy[\"AdoptionSpeed\"].value_counts()[0]*100)/df_train_copy.shape[0],\n",
    "               (df_train_copy[\"AdoptionSpeed\"].value_counts()[1]*100)/df_train_copy.shape[0],\n",
    "              (df_train_copy[\"AdoptionSpeed\"].value_counts()[2]*100)/df_train_copy.shape[0],\n",
    "              (df_train_copy[\"AdoptionSpeed\"].value_counts()[3]*100)/df_train_copy.shape[0],\n",
    "              (df_train_copy[\"AdoptionSpeed\"].value_counts()[4]*100)/df_train_copy.shape[0]]\n",
    "\n",
    "ax.pie(percentages, labels=labels,  \n",
    "        autopct='%1.0f%%', \n",
    "       shadow=False, startangle=0,   \n",
    "       pctdistance=1.2,labeldistance=0.8)\n",
    "ax.axis('equal')\n",
    "ax.set_title(\"AdoptionSpeed class distribution\")\n",
    "ax.legend(frameon=False, bbox_to_anchor=(1.5,0.8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "\n",
    "- From above distribution plot it is clear that data is **imbalanced** and there is only one class that is extreme minor i.e, 0 which counts for only 3% of all the data which is very very small. Rest of other 4 classes have very similar distribution. So we'll have have to take care of minor class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Our dataset is imbalanced, let's handle this problem later**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adoption_speed = df_train_copy['AdoptionSpeed'].value_counts()\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "sns.countplot(x='AdoptionSpeed', hue='Type', data=df_train_copy,palette = \"Set2\")\n",
    "\n",
    "plt.title('Distribution of Adoption Speed for Different Types')\n",
    "plt.xlabel('Adoption Speed')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that most pet were most not likely adopted the same day they were listed and dogs are more likely to get adopted than cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Pet Age feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='AdoptionSpeed', y='Age', data=df_train_copy, estimator='mean')\n",
    "plt.xlabel('Adoption Speed')\n",
    "plt.ylabel('Average Age')\n",
    "plt.title('Average Age vs. Adoption Speed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_train_copy.Age.value_counts()))\n",
    "df_train_copy.Age.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axe = plt.subplots()\n",
    "count_value = df_train_copy[\"Age\"].value_counts()\n",
    "sns.set_style('darkgrid')\n",
    "sns.barplot(x=count_value.values[0:10], y=count_value.index[0:10], orient=\"h\", ax=axe)\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Age\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From above we can take a conclusion that around 75% of pets are between 1 to 12 months old only.\n",
    "- It's a very commonsensical that age of pet is very important factor while adoption. So now we'll look at adoption speed with respect to age."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pet Gender Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy[\"Gender\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gender of pet \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_dog = df_train_copy[df_train_copy['Type'] == \"Dog\"]\n",
    "train_cat = df_train_copy[df_train_copy['Type'] == \"Cat\"]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 7)) \n",
    "\n",
    "plt.title(\"Genders of cats and dogs in train dataset\")\n",
    "\n",
    "train_dog['Gender'].value_counts().plot(kind='pie', figsize=(9, 9), ax=ax1, colors=['yellow', 'green'], autopct='%1.1f%%')\n",
    "train_cat['Gender'].value_counts().plot(kind='pie', figsize=(9, 9), ax=ax2, colors=['yellow', 'green'], autopct='%1.1f%%')\n",
    "\n",
    "ax1.set_xlabel('DOGS')\n",
    "ax2.set_xlabel('CATS')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gender distribution in test data is not bad. Females are greater in cats and dogs however the difference is not huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Breed Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train_copy['Breed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy['Breed'].value_counts().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the image above we can see that in the training data, the number of mixed_breed is the highest breed among dog while the domestic short hair is the highest among cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.groupby('Health')['Type'].value_counts().plot(kind='barh')\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(10,5))  # 1 row, 2 columns\n",
    "plt.title(\"healf by adoption speed  of  cats and dogs in TRAINING DATASET\")\n",
    "train_dog.groupby('Health')['AdoptionSpeed'].value_counts().plot(kind='bar',ax=ax1)\n",
    "train_cat.groupby('Health')['AdoptionSpeed'].value_counts().plot(kind='bar',ax=ax2)\n",
    "ax1.set_xlabel('DOGS')\n",
    "ax2.set_xlabel('CATS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heathy animals are most likely to get adopted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Fee Feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_copy.Fee.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Around 85% of total adoption posting have zero Fee.\n",
    "- And there seems some outlier because max fee value is 2000 and that seems alot. We'll do some analysis on fee values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.argsort(list(df_train_copy.Fee.value_counts().index))\n",
    "#plotting\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df_train_copy.Fee.value_counts().index[index],\n",
    "        np.cumsum(df_train_copy.Fee.value_counts().values[index])/np.sum(df_train_copy.Fee.value_counts().values[index]))\n",
    "plt.title(\"Adoption Fee\")\n",
    "plt.ylabel(\"% of total counts\")\n",
    "plt.xlabel(\"Adoption Fee\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So from above it is clear that 99.99% of adoption have Fee below 1000 so we can discard rest of entries with Fee above 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separating Numerical and Categorical columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to separate the numerical and categorical in different lists because later I would need to apply different trasformations to each type of column in the pipeline.\n",
    "I am not goind to take into account the ``target, description and images`` columns because they are not numerical or categorical (technically, the target is categorical but it does not matter as we want to predict it and will not use it as part of our input data). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not want ot change the original dataset so I will assign X (input data) and y (target) to different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train_copy.drop('AdoptionSpeed', axis=1).copy()\n",
    "y = np.array(df_train_copy['AdoptionSpeed']).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns_selector = selector(dtype_exclude=object) \n",
    "categorical_columns_selector = selector(pattern=r'^(?!.*(Description|Images))',dtype_include=object)\n",
    "numerical_columns = numerical_columns_selector(X)\n",
    "categorical_columns = categorical_columns_selector(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"List of numerical columns: \",numerical_columns)\n",
    "print(\"List of categorical columns: \", categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type is transformed converted it into cat = 0 and  dog=1  (Custom pipeline function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class TypeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['Type'] = X_transformed['Type'].map({'Dog': 1, 'Cat': 0})\n",
    "        return X_transformed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the custom pipeline\n",
    "# type_pipeline = Pipeline([\n",
    "#     ('type_converter', TypeConverter())\n",
    "# ])\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming df is your DataFrame with 'Gender' column\n",
    "# transformed_df = type_pipeline.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type_pipeline.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age is transformed  (Custom pipeline function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class AgeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, cat_scale=5, dog_scale=7):\n",
    "        \n",
    "        self.cat_scale = cat_scale\n",
    "        self.dog_scale = dog_scale\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \n",
    "        X_transformed = X.copy()\n",
    "        \n",
    "        # Apply the age conversion for cats\n",
    "        X_transformed.loc[X_transformed['Type'] == 'Cat', 'Age'] = \\\n",
    "            X_transformed.loc[X_transformed['Type'] == 'Cat', 'Age'] / self.cat_scale\n",
    "        \n",
    "        # Apply the age conversion for dogs\n",
    "        X_transformed.loc[X_transformed['Type'] == 'Dog', 'Age'] = \\\n",
    "            X_transformed.loc[X_transformed['Type'] == 'Dog', 'Age'] / self.dog_scale\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender = Male is converted to 0 and Female is converted to 1 (Custom pipeline function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "class GenderConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['Gender'] = X_transformed['Gender'].map({'Male': 1, 'Female': 0})\n",
    "        return X_transformed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the custom pipeline\n",
    "# gender_pipeline = Pipeline([\n",
    "#     ('gender_converter', GenderConverter())\n",
    "# ])\n",
    "\n",
    "# # Example usage:\n",
    "# # Assuming df is your DataFrame with 'Gender' column\n",
    "# transformed_df = gender_pipeline.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Mapping   (MaturitySize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinal encoding, is a process of transforming categorical variables into numerical variables while preserving the order or hierarchy of the categories. We can apply it to columns of our dataset which are categorical at a first glance but which some kind of progression can be identified in the unique values of the column (such as a progression in size, age, etc).\n",
    "\n",
    "To identify this, we need to study our columns' unique values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column ``MaturitySize`` is ordinal (there is a progression from small to extra large in its values), so we can map it as an ``ordinal column``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MaturitySize is converted into numerical  (Custom pipeline function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "\n",
    "class MaturitySizeConverter(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        size_mapping = {'Small': 0, 'Medium': 1, 'Large': 2, 'Extra Large': 3}\n",
    "        X_transformed['MaturitySize'] = X_transformed['MaturitySize'].map(size_mapping)\n",
    "        return X_transformed\n",
    "\n",
    "# Create the custom pipeline\n",
    "maturity_size_pipeline = Pipeline([\n",
    "    ('maturity_size_converter', MaturitySizeConverter())\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import nltk\n",
    "import emoji\n",
    "import string\n",
    "# from spellchecker import SpellChecker\n",
    "# from textblob import TextBlob\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# stops = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoji handling\n",
    "# !pip install emoji\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean description using TextCleaner  (Custom pipeline function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = X.copy()\n",
    "        X_transformed['Description'] = X_transformed['Description'].apply(self.clean_text1)\n",
    "        return X_transformed\n",
    "    \n",
    "    def clean_text1(self, text):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Initialize the WordNetLemmatizer\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        \n",
    "        # Remove emojis\n",
    "        emoji_pattern = re.compile(\"[\"\n",
    "                                    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                                    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                                    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                                    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                                    u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n",
    "                                    u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n",
    "                                    u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n",
    "                                    u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "                                    u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n",
    "                                    u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n",
    "                                    u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "                                    u\"\\U000024C2-\\U0001F251\" \n",
    "                                    u\"\\U0001F910-\\U0001F97A\"\n",
    "                                    u\"\\U0001F980-\\U0001F9B0\"\n",
    "                                    u\"\\U0001F9C0-\\U0001F9FF\"\n",
    "                                    u\"\\U0001FA60-\\U0001FA6F\"\n",
    "                                    u\"\\U0001FA70-\\U0001FAFF\"\n",
    "                                    u\"\\U00002600-\\U000026FF\"  # Miscellaneous Symbols\n",
    "                                    u\"\\U00002700-\\U000027BF\"  # Dingbats\n",
    "                                    u\"\\U00002B50\"\n",
    "                                    u\"\\U000023E9-\\U000023FA\"  # Miscellaneous Technical\n",
    "                                    \"]+\", flags=re.UNICODE)\n",
    "        text = emoji_pattern.sub(r'', text)\n",
    "\n",
    "        # Tokenize the text\n",
    "        text_tokens = word_tokenize(text)\n",
    "        \n",
    "        # Lemmatize the tokens and remove stop words\n",
    "        lemmatized_text = [lemmatizer.lemmatize(word.lower()) for word in text_tokens if word.lower() not in stop_words]\n",
    "        \n",
    "        # Join the lemmatized tokens back into a sentence\n",
    "        cleaned_sentence = ' '.join(lemmatized_text)\n",
    "        \n",
    "        # Remove punctuation and extra whitespace\n",
    "        cleaned_sentence = re.sub(r'[^\\w\\s]', '', cleaned_sentence).strip()\n",
    "        \n",
    "        return cleaned_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  IMAGE PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first image of the list\n",
    "from skimage import io\n",
    "\n",
    "\n",
    "img = io.imread(X['Images'][10])\n",
    "# have a look to the image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install opencv-contrib-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the image to grey levels \n",
    "import cv2\n",
    "\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SIFT detector and descriptors\n",
    "sift = cv2.SIFT_create()\n",
    "kp,des = sift.detectAndCompute(gray,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot image and descriptors\n",
    "cv2.drawKeypoints(img,kp,img,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract features and build BOFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator,TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from sklearn.cluster import MiniBatchKMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIFT (Scale-Invariant Feature Transform) is a feature extraction technique used in computer vision and image processing for detecting and describing local features in images. SIFT features are useful for a wide range of applications such as object recognition, image registration, 3D reconstruction, and image retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code performs the following steps:\n",
    "\n",
    "- ``Extracts Scale-Invariant Feature Transform (SIFT)`` features from a list of images and returns a list of these features.\n",
    "\n",
    "- ``Builds a clusterizer`` (a clustering algorithm) using the list of extracted SIFT features and a desired number of clusters. The clusterizer is then fitted to the SIFT features and returned.\n",
    "\n",
    "- ``Constructs a Bag of Features (BOF)`` representation using the list of SIFT features and the fitted clusterizer. The BOF representation is a histogram of the frequency of each cluster (i.e., visual word) in the SIFT features of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First step, extract the SIFTs of each image\n",
    "\n",
    "def extract_SIFT(img_lst):\n",
    "    nbSIFTs = 0    # Nomber of SIFTs\n",
    "    SIFTs = []  # List of SIFTs descriptors \n",
    "    #dimImgs = []   # Nb of descriptors associated to each images\n",
    "\n",
    "    for pathImg in tqdm(img_lst, position=0, leave=True): \n",
    "        img = io.imread(pathImg)\n",
    "        if len(img.shape)==2: # this is a grey level image\n",
    "            gray = img\n",
    "        else: # we expect the image to be a RGB image or RGBA\n",
    "            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        sift = cv2.SIFT_create()\n",
    "        kp, des = sift.detectAndCompute(gray, None)\n",
    "        if len(kp) == 0 and img.shape[2]==4: #some images are mask on alpha channel: we thus extract this channel if not kpts have been detected\n",
    "            gray = img[:,:,3]\n",
    "            sift = cv2.SIFT_create()\n",
    "            kp, des = sift.detectAndCompute(gray, None)\n",
    "        \n",
    "        nbSIFTs += des.shape[0]\n",
    "        SIFTs.append(des)\n",
    "        #dimImgs.append(des.shape[0])\n",
    "    return nbSIFTs, SIFTs#, dimImgs\n",
    "\n",
    "\n",
    "# Step 2: clusterize the SIFT\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "# performs clustering on SIFT descriptors using the MiniBatchKMeans algorithm he goal of \n",
    "#clustering is to group similar descriptors together and reduce the dimensionality of the feature space.\n",
    "\n",
    "def clusterize(SIFTs, nb_img_features=5, verbose=False):\n",
    "    clusterizer = MiniBatchKMeans(n_clusters=nb_img_features)   # nb_img_features is a hyperparameter\n",
    "    # learning of the clustering\n",
    "    flat_list = SIFTs[0]\n",
    "    for des in SIFTs[1:]:\n",
    "        flat_list = np.concatenate((flat_list, des)) # concate\n",
    "        if verbose:\n",
    "            print(\"shape:\", des.shape, flat_list.shape)\n",
    "    clusterizer.fit(flat_list)\n",
    "    # we now know the label of each SIFT descriptor\n",
    "    return clusterizer\n",
    "\n",
    "\n",
    "# Step 3: build the BOW representation of each images (i.e. construction of the BOFs)\n",
    "\n",
    "def build_BOFs(SIFTs, clusterizer, verbose=False):\n",
    "    ok, nok = 0, 0\n",
    "    #BOF initialization\n",
    "    nb_img_features = clusterizer.get_params()['n_clusters']# Get the number of clusters from the KMeans clusterizer\n",
    "    BOFs = np.empty(shape=(0, nb_img_features), dtype=int) #Initialize an empty numpy array with the appropriate shape and dtype for the BOFs\n",
    "\n",
    "    # Build label list\n",
    "    flat_list = SIFTs[0]\n",
    "    # Iterate over the remaining images in the dataset and concatenate their SIFT features into flat_list\n",
    "    for des in SIFTs[1:]:\n",
    "        flat_list = np.concatenate((flat_list, des))\n",
    "        if verbose:\n",
    "            print(\"shape:\", des.shape, flat_list.shape)\n",
    "    labels = clusterizer.predict(flat_list)#Use the clusterizer to predict the cluster labels for each SIFT feature in flat_list\n",
    "\n",
    "    # loop on images\n",
    "    i = 0 # index for the loop on SIFTs\n",
    "    for des in SIFTs:\n",
    "        #initialisation of the bof for the current image\n",
    "        tmpBof = np.array([0]*nb_img_features)\n",
    "        j = 0\n",
    "        # for every SIFT of the current image:\n",
    "        nbs = des.shape[0]\n",
    "        while j < nbs:\n",
    "            tmpBof[labels[i]] += 1\n",
    "            j+=1\n",
    "            i+=1\n",
    "        BOFs = np.concatenate((BOFs, tmpBof.reshape(1,-1)), axis=0)\n",
    "    if verbose:\n",
    "        print(\"BOFs : \", BOFs)\n",
    "    \n",
    "    return BOFs\n",
    "\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "\n",
    "\n",
    "def list_comparaison(l1, l2):\n",
    "    if not l1 is None \\\n",
    "        and not l2 is None \\\n",
    "        and len(l1)==len(l2) \\\n",
    "        and len(l1)==sum([1 for i,j in zip(l1, l2) if i==j]):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "class BOF_extractor(BaseEstimator,TransformerMixin):\n",
    "    X = None\n",
    "    SIFTs = None\n",
    "    nbSIFTs = 0\n",
    "\n",
    "    def __init__(self, nb_img_features=5, verbose=False):\n",
    "        self.nb_img_features = nb_img_features\n",
    "        self.verbose = verbose\n",
    "       # self.path = project_path\n",
    "        if self.verbose:\n",
    "            print(\"BOF.init()\")\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        if self.verbose:\n",
    "            print(\"BOF.fit()\")\n",
    "        if list_comparaison(X, self.X):\n",
    "            SIFTs = self.SIFTs\n",
    "            nbSIFTs = self.nbSIFTs\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"extract_SIFT\")\n",
    "            nbSIFTs, SIFTs = extract_SIFT(X)\n",
    "        self.X = X\n",
    "        self.SIFTs = SIFTs\n",
    "        self.nbSIFTs = nbSIFTs\n",
    "        self.clusterizer = clusterize(SIFTs, self.nb_img_features, self.verbose)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        if self.verbose:\n",
    "            print(\"BOF.transform()\")\n",
    "        if list_comparaison(X, self.X):\n",
    "            SIFTs = self.SIFTs\n",
    "            nbSIFTs = self.nbSIFTs\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"extract_SIFT\")\n",
    "            nbSIFTs, SIFTs = extract_SIFT(X)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"nbSIFTs:\", nbSIFTs)\n",
    "        return build_BOFs(SIFTs, self.clusterizer, self.verbose)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        if self.verbose:\n",
    "            print(\"BOF.fit_transform()\")\n",
    "        if list_comparaison(X, self.X):\n",
    "            SIFTs = self.SIFTs\n",
    "            nbSIFTs = self.nbSIFTs\n",
    "        else:\n",
    "            if self.verbose:\n",
    "                print(\"extract_SIFT\")\n",
    "            nbSIFTs, SIFTs = extract_SIFT(X)\n",
    "        self.X = X\n",
    "        self.SIFTs = SIFTs\n",
    "        self.nbSIFTs = nbSIFTs\n",
    "        self.clusterizer = clusterize(SIFTs, self.nb_img_features, self.verbose)\n",
    "        return build_BOFs(SIFTs, self.clusterizer, self.verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing Pipeline  Create X and y(Target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve accurate machine learning results, it's important to use a balanced dataset. This means that the number of samples for each class in the dataset should be roughly the same. If the dataset is imbalanced, the machine learning model may be biased towards the majority class, leading to poor performance on the minority class.\n",
    "\n",
    "To address this issue, my approach is to balance the dataset using a technique like RandomOverSampler.\n",
    "\n",
    "- https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing ``cross-validation`` is a powerful method to evaluate the performance of a machine learning model. However, it can be computationally expensive, especially when the dataset is large. In this case, In my case I am facing a time constraint, and the computer takes a long time to perform cross-validation.\n",
    "\n",
    "- https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "To optimize my time, I am planning to only perform cross-validation on the balanced dataset since this is the recommended approach. I will not perform cross-validation on the non-balanced dataset because I only want to compare its accuracy with the accuracy of the balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spliting the data into dependent and independent varaible\n",
    "\n",
    "y = df_train_copy['AdoptionSpeed']\n",
    "X = df_train_copy.drop(['AdoptionSpeed'], axis=1).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "features = X.columns.tolist()\n",
    "\n",
    "numerical_features = [feature for feature in features if \n",
    "                X[feature].dtype != 'object' or feature == 'MaturitySize' or feature ==  'Gender' or feature == \"Type\" ]\n",
    "categorical_features = [feature for feature in features if\n",
    "                X[feature].dtype == object and feature != \"MaturitySize\" and  feature != \"Gender\"   and  feature != \"Description\" and  feature != \"Images\"  and  feature != \"Color3\" and  feature != \"Type\" ]\n",
    "\n",
    "\n",
    "print(numerical_features)\n",
    "# print(\"\\n\")\n",
    "\n",
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Assuming energy_df_id_remove_2 is your DataFrame and features is a list of column names\n",
    "# features = X.columns.tolist()\n",
    "\n",
    "# numerical_features = [feature for feature in features if \n",
    "#                 X[feature].dtype != 'object' ]\n",
    "# categorical_features = [feature for feature in features if\n",
    "#                 X[feature].dtype == object   and  feature != \"Description\" and  feature != \"Images\" ]\n",
    "\n",
    "\n",
    "# print(numerical_features)\n",
    "# # print(\"\\n\")\n",
    "\n",
    "# print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_features = ['Description']\n",
    "image_features = ['Images']\n",
    "drop_features = [\"Color3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  checking the number of observations per classes\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://scikit-learn.org/1.4/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating pipelines for numerical and categorical features\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('stdscaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encoding categorical features\n",
    "])\n",
    "\n",
    "# Define pipeline for text features\n",
    "text_transformer_pipeline = Pipeline(steps=[ ('vectorizer', CountVectorizer())])\n",
    "image_transformer_pipeline = Pipeline(steps=[ ('img', BOF_extractor())])\n",
    "\n",
    "\n",
    "\n",
    "col_transformer = ColumnTransformer(transformers=[\n",
    "   ('drop_columns','drop',drop_features),\n",
    "\n",
    "    ('numerical', numerical_pipeline, numerical_features),\n",
    "    ('categorical', categorical_pipeline, categorical_features),\n",
    "    \n",
    "    ('text-data', text_transformer_pipeline, 'Description'),\n",
    "    ('image-data', image_transformer_pipeline, 'Images'),\n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "    \n",
    "my_sklearn_pipeline = Pipeline(steps=[\n",
    "    ('TypeConverter', TypeConverter()),\n",
    "    ('age', AgeConverter()),\n",
    "    ('text_cleaner', TextCleaner()),\n",
    "    ('MaturitySizeConverter' ,MaturitySizeConverter()),\n",
    "    (\"GenderConverter\",GenderConverter()),\n",
    "\n",
    "                 ('col_transformer',col_transformer),\n",
    "    ]\n",
    "\n",
    "                           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_sklearn_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_final = Pipeline(steps=[\n",
    "#     ('pipe',sklearn_pipeline),\n",
    "#                           ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomOverSampler() is used to balance the data by oversampling the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "\n",
    "#  oversampling the minority class of the dataset\n",
    "ros=RandomOverSampler(sampling_strategy='minority')\n",
    "\n",
    "X_sampled,y_sampled=ros.fit_resample(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_sampled[\"Images\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # New observation distribution\n",
    "\n",
    "pd.Series(y_sampled).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I will use this resampled data set for my model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split the balanced dataset with only 1000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation set \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_1000, X_val_1000, y_train_1000, y_val_1000 = train_test_split(X_sampled[:1000], y_sampled[:1000], test_size=0.20, random_state=2022,stratify=y_sampled[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The different models with the best hyper parameters, after the cross validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomizedSearchCV I have done only with the balanced dataset, I haven't try even with the non balanced dataset because in my computer it takes more than one day and sometimes it froze and I needed to switch off the computer and start again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation with RandomizedSearchCV is a technique used to optimize the hyperparameters of a machine learning model using randomized search. RandomizedSearchCV is a function from the scikit-learn library that performs hyperparameter tuning using a combination of random search and cross-validation.\n",
    "\n",
    "The process works by defining a range of hyperparameters and their potential values. RandomizedSearchCV then randomly selects a combination of hyperparameter values from this range and trains and evaluates the model using cross-validation. The process is repeated multiple times, each time selecting a different set of hyperparameter values.\n",
    "\n",
    "The advantage of RandomizedSearchCV is that it can be more efficient than an exhaustive grid search of all possible hyperparameter combinations. By randomly sampling the hyperparameter space, RandomizedSearchCV can often find good hyperparameter values with fewer evaluations.\n",
    "\n",
    "The output of RandomizedSearchCV is the best set of hyperparameters that was found during the search, along with the corresponding cross-validation score. These hyperparameters can then be used to train the final model on the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Models where I will train with balance dataset  (RandomizedSearchCV )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define algorithms and their corresponding hyperparameters\n",
    "param_distributions = {\n",
    "    'RandomForestClassifier': {'randomforestclassifier__n_estimators': [50, 150],\n",
    "                               'randomforestclassifier__max_depth': [15, 50], \n",
    "                               'randomforestclassifier__bootstrap': [True, False],\n",
    "                              },\n",
    "    'GradientBoostingClassifier': {'gradientboostingclassifier__n_estimators': [80, 100, 120], 'gradientboostingclassifier__learning_rate': [0.1, 0.2, 0.3]},\n",
    "    'KNN_CLF': {'kneighborsclassifier__n_neighbors': [4, 5, 6]},\n",
    "    'LogisticRegression': {'logisticregression__penalty':['l1', 'l2', 'elasticnet'], \n",
    "                           'logisticregression__C': [0.2, 0.5, 1.0, 2.0], \n",
    "                           'logisticregression__verbose': [1, 2, 3],\n",
    "                          'logisticregression__max_iter': [1000] }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'GradientBoostingClassifier': make_pipeline(my_sklearn_pipeline, GradientBoostingClassifier()),\n",
    "    'KNN_CLF': make_pipeline(my_sklearn_pipeline, KNeighborsClassifier()),\n",
    "    'LogisticRegression': make_pipeline(my_sklearn_pipeline, LogisticRegression()),\n",
    "    'RandomForestClassifier': make_pipeline(my_sklearn_pipeline, RandomForestClassifier())\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform random search for each algorithm\n",
    "best_params = {}\n",
    "for algo, clf in classifiers.items():\n",
    "    search = RandomizedSearchCV(clf, param_distributions[algo], n_iter=10, cv=5, random_state=42)\n",
    "    search.fit(X_train_1000, y_train_1000)\n",
    "    best_params[algo] = search.best_params_\n",
    "\n",
    "# Print best parameters for each algorithm\n",
    "for algo, params in best_params.items():\n",
    "    print(f\"Best parameters for {algo}: {params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Best Parameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best parameters for GradientBoostingClassifier: {'gradientboostingclassifier__n_estimators': 100, 'gradientboostingclassifier__learning_rate': 0.1}\n",
    "Best parameters for KNN_CLF: {'kneighborsclassifier__n_neighbors': 5}\n",
    "Best parameters for LogisticRegression: {'logisticregression__verbose': 2, 'logisticregression__penalty': 'l2', 'logisticregression__max_iter': 1000, 'logisticregression__C': 2.0}\n",
    "Best parameters for RandomForestClassifier: {'randomforestclassifier__n_estimators': 50, 'randomforestclassifier__max_depth': 15, 'randomforestclassifier__bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training using the whole Resampled train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and validation set \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_sampled, y_sampled, test_size=0.20, random_state=2022,stratify=y_sampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model training using LogisticRegression with best_params \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_lr = LogisticRegression()# default parameters, C = 1.0\n",
    "\n",
    "model_lr = make_pipeline(my_sklearn_pipeline, classifier_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_lr={'logisticregression__verbose': 2, 'logisticregression__penalty': 'l2', 'logisticregression__max_iter': 1000, 'logisticregression__C': 2.0\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_lr = model_lr.set_params(**best_params_lr)\n",
    "pred_lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_lr=pred_lr.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_val, y_pred_lr, average= 'weighted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "kp_lr=cohen_kappa_score(y_val,y_pred_lr)\n",
    "kp_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model training using RandomForestClassifier with best_params \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_rf = RandomForestClassifier()# default parameters, C = 1.0\n",
    "\n",
    "model_rf = make_pipeline(my_sklearn_pipeline, classifier_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_rf= {'randomforestclassifier__n_estimators': 50, 'randomforestclassifier__max_depth': 15, 'randomforestclassifier__bootstrap': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_rf=model_rf.set_params(**best_params_rf)\n",
    "pred_rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf=pred_rf.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_val, y_pred_rf, average= 'weighted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "kp_rf=cohen_kappa_score(y_val,y_pred_rf)\n",
    "kp_rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model training using GradientBoostingClassifier with best_params \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_gb = GradientBoostingClassifier()\n",
    "\n",
    "model_gb = make_pipeline(my_sklearn_pipeline, classifier_gb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_gb= {'gradientboostingclassifier__n_estimators': 100, 'gradientboostingclassifier__learning_rate': 0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_gb=model_gb.set_params(**best_params_gb)\n",
    "pred_gb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gb=pred_gb.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_val, y_pred_gb, average= 'weighted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "kp_gb=cohen_kappa_score(y_val,y_pred_gb)\n",
    "kp_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model training using KNeighborsClassifier with best_params \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_knn = KNeighborsClassifier()\n",
    "model_knn = make_pipeline(my_sklearn_pipeline, classifier_knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params_knn= {'kneighborsclassifier__n_neighbors': 5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_knn=model_knn.set_params(**best_params_knn)\n",
    "pred_knn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_knn=pred_knn.predict(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_val, y_pred_knn, average= 'weighted') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "kp_knn=cohen_kappa_score(y_val,y_pred_knn)\n",
    "kp_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Asthe highest kappa score I found by using Logistic Regression so finally I will use it  for the test data prediction**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_copy[\"Images\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Like I said before, I had the best kappa score with Logistic Regression I choose it as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_logistic_true = model_lr.predict(df_test_copy)\n",
    "# y_pred_random_true = model_random_true.predict(X_test_true)\n",
    "# y_pred_svc_true = model_svc_true.predict(X_test_true)\n",
    "# y_pred_knn_true = model_knn_true.predict(X_test_true)\n",
    "# y_pred_gbc_true = model_gbc_true.predict(X_test_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(y_pred_logistic_true)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('results_.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
