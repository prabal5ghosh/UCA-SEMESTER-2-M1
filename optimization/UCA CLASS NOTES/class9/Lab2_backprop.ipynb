{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbzBJ1m9FBBb"
      },
      "source": [
        "![UCA](https://univ-cotedazur.fr/medias/fichier/uca-logo-ligne-mono-bleu_1594383427225-)\n",
        "# **Lab session 2: Backpropagation on neural networks**\n",
        "\n",
        "## **Course: Optimization for data science**\n",
        "\n",
        "\n",
        "#### Lab session proposed by\n",
        "------------------------------------------------\n",
        "\n",
        "### Rémy Sun\n",
        "# Warning :\n",
        "# \"File -> Save a copy in Drive\" before starting to modify the notebook, otherwise changes won't be saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfnKy8NB8J5e"
      },
      "source": [
        "!wget https://remysun.github.io/uploads/TPBackprop.zip\n",
        "!unzip -j TPBackprop.zip\n",
        "!wget https://remysun.github.io/uploads/utils-data.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vQ_LLdx8J5b"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%run 'utils-data.py'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48x_ha7f8J5i"
      },
      "source": [
        "# Part 1 : Fully manual backprop\n",
        "\n",
        "We start by initializing the weights! It is done for you here, no need to code anything.\n",
        "\n",
        "**Question: Why do we initialize with random values here?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtizX1JV8J5n"
      },
      "source": [
        "def init_params(nx, nh, ny):\n",
        "    \"\"\"\n",
        "    nx, nh, ny: integers\n",
        "    out params: dictionnary\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "\n",
        "    params[\"Wh\"] = torch.randn((nh, nx))*0.3\n",
        "    params[\"Wy\"] = torch.randn((ny, nh))*0.3\n",
        "    params[\"bh\"] = torch.zeros((nh,1))\n",
        "    params[\"by\"] = torch.zeros((ny,1))\n",
        "\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to put the weight together into an actual neural network. Implement the forward function."
      ],
      "metadata": {
        "id": "EJeZjnWtEbdP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk-N_Ny67yo-"
      },
      "source": [
        "def forward(params, X):\n",
        "    \"\"\"\n",
        "    params: dictionnary\n",
        "    X: (n_batch, dimension)\n",
        "    \"\"\"\n",
        "    bsize = X.size(0)\n",
        "    nh = params['Wh'].size(0)\n",
        "    ny = params['Wy'].size(0)\n",
        "    outputs = {}\n",
        "\n",
        "    outputs[\"X\"] = X\n",
        "    outputs[\"htilde\"] = torch.mm(X, params[\"Wh\"].T) + params[\"bh\"].T\n",
        "    outputs[\"h\"] = torch.tanh(outputs[\"htilde\"])\n",
        "    outputs[\"ytilde\"] = torch.mm(outputs[\"h\"], params[\"Wy\"].T) + params[\"by\"].T\n",
        "    outputs[\"yhat\"] = torch.exp(outputs[\"ytilde\"])\n",
        "    outputs[\"yhat\"] = outputs[\"yhat\"] / outputs[\"yhat\"].sum(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "    return outputs['yhat'], outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "An additional part of computations is also to compute the loss functions, but this has little bearing on this lab session so it is done for you here."
      ],
      "metadata": {
        "id": "Sz1bXv3YEjHl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-uB0A2b28NZK"
      },
      "source": [
        "def loss_accuracy(Yhat, Y):\n",
        "\n",
        "\n",
        "    L = - torch.mean((Y * torch.log(Yhat)).sum(dim=1)) # mean for the batch\n",
        "\n",
        "    _, indYhat = torch.max(Yhat, 1)\n",
        "    _, indY = torch.max(Y, 1)\n",
        "\n",
        "    acc = torch.sum(indY == indYhat) * 100. / indY.size(0);\n",
        "\n",
        "\n",
        "    return L, acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we need to implement the actual backpropagation algorithm: time to get the gradients!\n",
        "\n",
        "**Question: How do you get gradients for the different weights?**"
      ],
      "metadata": {
        "id": "Tc5fiY-BEqo1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWJjdiFe8qi5"
      },
      "source": [
        "def backward(params, outputs, Y):\n",
        "    bsize = Y.shape[0]\n",
        "    grads = {}\n",
        "\n",
        "    grads[\"Wy\"] = None\n",
        "    grads[\"Wh\"] = None\n",
        "    grads[\"by\"] = None\n",
        "    grads[\"bh\"] = None\n",
        "\n",
        "    return grads"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we have to implement and SGD step, which you already did in the last practical."
      ],
      "metadata": {
        "id": "ECoA3yFeE50a"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAnsISsW9CnH"
      },
      "source": [
        "def sgd(params, grads, eta):\n",
        "\n",
        "    params['Wy'] = None\n",
        "    params['Wh'] = None\n",
        "    params['by'] = None\n",
        "    params['bh'] = None\n",
        "\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hifuW5UFA3DZ"
      },
      "source": [
        "## Manual global learning algorithm\n",
        "\n",
        "We now put everything together to train the model!\n",
        "\n",
        "**Question: Describe what happens during trainig (comment on the colors for instance)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RSw6bd0-qUe"
      },
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "params = init_params(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        Y_hat, outputs = None\n",
        "        loss, accuracy = None\n",
        "        grads = None\n",
        "        params = None\n",
        "\n",
        "\n",
        "    Yhat_train, _ = forward(params, data.Xtrain)\n",
        "    Yhat_test, _ = forward(params, data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "    Ygrid, _ = forward(params, data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title)\n",
        "    data.plot_data_with_grid(Ygrid, title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain)\n",
        "    curves[3].append(Ltest)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrHHH5PL8J54"
      },
      "source": [
        "# Part 2 : Simplify backward with `torch.autograd`\n",
        "\n",
        "`torch.autograd` saves us a lot of hassle by computing gradients for us!\n",
        "\n",
        "For that however, we need to tell pytorch the parameter tensors are part of a tracked computational graph by specifying they require gradients.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G4q5zP0CEvB"
      },
      "source": [
        "def init_params(nx, nh, ny):\n",
        "    \"\"\"\n",
        "    nx, nh, ny: integers\n",
        "    out params: dictionnary\n",
        "    \"\"\"\n",
        "    params = {}\n",
        "\n",
        "    params[\"Wh\"] = torch.randn(nh, nx) * 0.3\n",
        "    params[\"Wh\"].requires_grad = True\n",
        "    params[\"Wy\"] = torch.randn(ny, nh) * 0.3\n",
        "    params[\"Wy\"].requires_grad = True\n",
        "\n",
        "    params[\"bh\"] = torch.zeros(nh,1, requires_grad=True)\n",
        "    params[\"by\"] = torch.zeros(ny,1, requires_grad=True)\n",
        "\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL0tSjpKCyVB"
      },
      "source": [
        "`forward` and `loss_accuracy` are the same as before.\n",
        "\n",
        "`backward` is not necessary anymore thanks to pytorch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA4ycHlfBzCK"
      },
      "source": [
        "def sgd(params, eta):\n",
        "\n",
        "    #####################\n",
        "    ## Your code here  ##\n",
        "    #####################\n",
        "    # Update parameters with a SGD step\n",
        "    # Take care to take this off the computation graph with torch.no_grad()\n",
        "    # And remember to reset the gradients after use with .grad.zero_()\n",
        "\n",
        "    params[\"Wh\"] = None\n",
        "    params[\"Wy\"] = None\n",
        "    params[\"bh\"] = None\n",
        "    params[\"by\"] = None\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "    return params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we put it all together in a training loop again.\n",
        "\n",
        "**Question: What is the difference this time?**"
      ],
      "metadata": {
        "id": "vzVYY7LSHipj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8p5oR3EqDea-"
      },
      "source": [
        "# init\n",
        "data = CirclesData()\n",
        "data.plot_data()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 10\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 10\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03\n",
        "\n",
        "params = init_params(nx, nh, ny)\n",
        "\n",
        "curves = [[],[], [], []]\n",
        "\n",
        "# epoch\n",
        "for iteration in range(150):\n",
        "\n",
        "    # permute\n",
        "    perm = np.random.permutation(N)\n",
        "    Xtrain = data.Xtrain[perm, :]\n",
        "    Ytrain = data.Ytrain[perm, :]\n",
        "\n",
        "    #####################\n",
        "    ## Your code here  ##\n",
        "    #####################\n",
        "    # batches\n",
        "    for j in range(N // Nbatch):\n",
        "\n",
        "        indsBatch = range(j * Nbatch, (j+1) * Nbatch)\n",
        "        X = Xtrain[indsBatch, :]\n",
        "        Y = Ytrain[indsBatch, :]\n",
        "\n",
        "        # Write out the training code on batch inputs (X,Y)\n",
        "        # Use our new forward, loss_accuracy, sgd\n",
        "        # Gradients are taken care of by .backward() and .grad\n",
        "\n",
        "\n",
        "    ####################\n",
        "    ##      FIN        #\n",
        "    ####################\n",
        "\n",
        "\n",
        "    Yhat_train, _ = forward(params, data.Xtrain)\n",
        "    Yhat_test, _ = forward(params, data.Xtest)\n",
        "    Ltrain, acctrain = loss_accuracy(Yhat_train, data.Ytrain)\n",
        "    Ltest, acctest = loss_accuracy(Yhat_test, data.Ytest)\n",
        "    Ygrid, _ = forward(params, data.Xgrid)\n",
        "\n",
        "    title = 'Iter {}: Acc train {:.1f}% ({:.2f}), acc test {:.1f}% ({:.2f})'.format(iteration, acctrain, Ltrain, acctest, Ltest)\n",
        "    print(title)\n",
        "    # detach() est utilisé pour détacher les predictions du graphes de calcul autograd\n",
        "    data.plot_data_with_grid(Ygrid.detach(), title)\n",
        "\n",
        "    curves[0].append(acctrain)\n",
        "    curves[1].append(acctest)\n",
        "    curves[2].append(Ltrain)\n",
        "    curves[3].append(Ltest)\n",
        "\n",
        "fig = plt.figure()\n",
        "plt.plot(curves[0], label=\"acc. train\")\n",
        "plt.plot(curves[1], label=\"acc. test\")\n",
        "plt.plot(curves[2], label=\"loss train\")\n",
        "plt.plot(curves[3], label=\"loss test\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FV1iss68J6H"
      },
      "source": [
        "# Part 3 : Deeper neural network (Bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6T5Uq7JEl47"
      },
      "source": [
        "Fundamentally, adding an additional hidden layer would just add some computations in the forward and backward along with new weights.\n",
        "\n",
        "Can you adapt the previous code and derivations to train a three layer neural network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ts1s4JuOSaZ3"
      },
      "source": [
        "# Part 4 : MNIST (Bonus)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jly9C4FCSzLP"
      },
      "source": [
        "\n",
        "Adapt the previous code to the MNIST dataset MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osrFoEr_Syi7"
      },
      "source": [
        "# init\n",
        "data = MNISTData()\n",
        "N = data.Xtrain.shape[0]\n",
        "Nbatch = 100\n",
        "nx = data.Xtrain.shape[1]\n",
        "nh = 100\n",
        "ny = data.Ytrain.shape[1]\n",
        "eta = 0.03"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}